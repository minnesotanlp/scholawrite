<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <!--css from bootstrap and font-awesome-->
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
      
      <!--css from pdf.js and linghe-->
      <link rel="stylesheet" href="/scholawrite/static/css/viewer.css">
      <link rel="stylesheet" href="/scholawrite/static/css/latex_replay.css">

      <!--js from bootstrap-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.slim.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
      <script>
         let llama3Labels = [{'label': 'Showing Seed'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}, {'label': 'Object Insertion'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Section Planning'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}, {'label': 'Coherence'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}, {'label': 'Coherence'}, {'label': 'Linguistic Style'}, {'label': 'Clarity'}, {'label': 'Object Insertion'}, {'label': 'Idea Generation'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Visual Formatting'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Visual Formatting'}, {'label': 'Text Production'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Section Planning'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Linguistic Style'}, {'label': 'Clarity'}, {'label': 'Object Insertion'}, {'label': 'Text Production'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Coherence'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Object Insertion'}, {'label': 'Idea Generation'}, {'label': 'Linguistic Style'}, {'label': 'Cross-reference'}, {'label': 'Structural'}, {'label': 'Coherence'}, {'label': 'Idea Generation'}, {'label': 'Coherence'}, {'label': 'Object Insertion'}, {'label': 'Visual Formatting'}, {'label': 'Idea Generation'}, {'label': 'Section Planning'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Visual Formatting'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Linguistic Style'}, {'label': 'Text Production'}, {'label': 'Object Insertion'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Visual Formatting'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}]
         let llama3Revisions = [{'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>First, we propose a persuasion taxonomy derived from decades of social science research.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % </span><del style="background:#F1948A;">R</del><ins style="background:#82E0AA;">r</ins><span>equired for inserting images&para;<br>&para;<br>\\title{How </span><del style="background:#F1948A;">Johnny Can Persuade LLMs to Jailbreak</del><ins style="background:#82E0AA;">LLMs</ins><span> The</span><del style="background:#F1948A;">m</del><ins style="background:#82E0AA;">y</ins><span>: &para;<br>\\\\Rethinking Persu</span><del style="background:#F1948A;">as</del><ins style="background:#82E0AA;">pt</ins><span>ion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by </span><del style="background:#F1948A;">security</del><ins style="background:#82E0AA;">pr</ins><span> experts. As \\textit{large language models} (LLMs) become increasingly common and c</span><del style="background:#F1948A;">ompetent,</del><ins style="background:#82E0AA;">apable, this</ins><span> non-expert users can also </span><del style="background:#F1948A;">im</del><span>pose risks during </span><ins style="background:#82E0AA;">every</ins><span>da</span><del style="background:#F1948A;">il</del><span>y interactions. This paper introduces a new perspective on </span><del style="background:#F1948A;">jailbreaking LLMs as human-like communicators to</del><ins style="background:#82E0AA;">L</ins><span>  &para;<br>explore this overlooked inter</span><del style="background:#F1948A;">se</del><ins style="background:#82E0AA;">a</ins><span>ction between everyday language </span><del style="background:#F1948A;">interaction </del><span>and AI safety. Specifically, we study how to </span><del style="background:#F1948A;">persuad</del><span>e LLMs to </span><del style="background:#F1948A;">jailbreak</del><ins style="background:#82E0AA;">b</ins><span> them. &para;<br>First, we propose a </span><del style="background:#F1948A;">persuasion taxonomy derived</del><ins style="background:#82E0AA;">\\textit{pr lexicality resulting</ins><span> from </span><del style="background:#F1948A;">de</del><span>cad</span><del style="background:#F1948A;">e</del><span>s </span><del style="background:#F1948A;">of social science research.&para;<br></del><ins style="background:#82E0AA;">es. </ins><span>Then we apply the </span><del style="background:#F1948A;">taxonomy</del><ins style="background:#82E0AA;">\\textit{lexi</ins><span> to automatically generate &para;<br></span><del style="background:#F1948A;">interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>Results show that persuasion</del><ins style="background:#82E0AA;">e jailbreak LLMs. &para;<br>h persuiveness</ins><span> significantly increases the </span><del style="background:#F1948A;">jailbreak</del><ins style="background:#82E0AA;">\\textit{i</ins><span> performance across all risk categories: PAP </span><del style="background:#F1948A;">consistently achieves an attack success</del><ins style="background:#82E0AA;">h attacks e</ins><span> rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ </span><del style="background:#F1948A;">trials, surpassing recent</del><ins style="background:#82E0AA;">r</ins><span> &para;<br>algorithm-</span><del style="background:#F1948A;">focu</del><ins style="background:#82E0AA;">ba</ins><span>sed attacks. &para;<br></span><del style="background:#F1948A;">On the defense side, w</del><span>e explore various </span><del style="background:#F1948A;">mechanism</del><ins style="background:#82E0AA;">ncan</ins><span>s against PAP, find a significant gap in existing </span><del style="background:#F1948A;">defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs</del><ins style="background:#82E0AA;">empirical advise d &para;<br>e</ins><span>.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}</span><ins style="background:#82E0AA;">&para;<br></ins>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><del style="background:#F1948A;">&nbsp;</del><span>They</span><del style="background:#F1948A;">:</del><ins style="background:#82E0AA;"> Perceive</ins><span> &para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by pr experts. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-expert users can also pose risks during everyday interactions. This paper introduces a new perspective on L  &para;<br>explore this overlooked interaction between everyday language and AI safety. Specifically, we study how to </span><del style="background:#F1948A;">e</del><ins style="background:#82E0AA;">implement</ins><span> LLMs to b</span><ins style="background:#82E0AA;">e</ins><span> them. &para;<br>First, we propose a \\textit{pr lexicality </span><ins style="background:#82E0AA;">score</ins><span>resulting from </span><del style="background:#F1948A;">cads es</del><ins style="background:#82E0AA;">a analysis of sentence-level structure</ins><span>. Then we apply the \\textit{lexi t</span><del style="background:#F1948A;">o </del><span>automatic</span><del style="background:#F1948A;">ally generate &para;<br>e</del><ins style="background:#82E0AA;">&nbsp;</ins><span> jailbreak</span><ins style="background:#82E0AA;">ing</ins><span> LLMs. &para;<br>h</span><del style="background:#F1948A;"> persuiveness significantly </del><ins style="background:#82E0AA;">trains a s</ins><span>increases the \\textit{i</span><ins style="background:#82E0AA;">ntra-model</ins><span> performance across all risk categories: PAP </span><del style="background:#F1948A;">h</del><ins style="background:#82E0AA;">and</ins><span> attacks e</span><del style="background:#F1948A;"> rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ r &para;<br></del><span>algorithm-based attacks. &para;<br>e explore various ncans against PAP, find a significant gap in existing empirical ad</span><del style="background:#F1948A;">v</del><span>ise </span><ins style="background:#82E0AA;">an</ins><span>d </span><del style="background:#F1948A;">&para;<br></del><span>e.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><ins style="background:#82E0AA;">&nbsp;</ins><span>They </span><del style="background:#F1948A;">P</del><ins style="background:#82E0AA;">p</ins><span>erceive &para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by pr experts. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-expert users can also pose risks during everyday interactions. This paper introduces a new perspective on L  &para;<br>explore this overlooked interaction between everyday language and AI safety. Specifically, we study how to implement LLMs to be them. &para;<br>First, we propose a \\textit{pr lexicality scorere</span><ins style="background:#82E0AA;">re</ins><span>sulting from a analysis of sentence-level structure. Then we apply the \\textit{lexi t</span><del style="background:#F1948A;">automatic  j</del><ins style="background:#82E0AA;"> computed lexical results to </ins><span>ail</span><ins style="background:#82E0AA;">&nbsp;</ins><span>breaking LLMs. &para;<br>htrains a sincreases the \\textit{intra-model performance across all risk categories: PAP and attacks ealgorithm-based attacks. &para;<br>e explore various ncans against PAP, find a significant gap in existing empirical adise and e.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><del style="background:#F1948A;">&nbsp;</del><span>They perceive &para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach</span><del style="background:#F1948A;">ed</del><span> AI models as machines and centered on algorithm-focused &para;<br>attacks developed by pr</span><del style="background:#F1948A;"> expert</del><ins style="background:#82E0AA;">oman</ins><span>s. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-</span><del style="background:#F1948A;">expert</del><ins style="background:#82E0AA;">anonymous</ins><span> users can also pose risks during everyday interactions. This paper introduces a new perspective on L  &para;<br>explore</span><ins style="background:#82E0AA;">in</ins><span> this overlooked interaction between everyday language and AI safety. Specifically, we study how to implement LLMs to be them. &para;<br>First, we propose a \\textit{pr lexicality scorereresulting from a analysis of sentence-level structure. Then we apply the \\textit{lexi t computed lexical results to ail breaking LLMs. &para;<br></span><del style="background:#F1948A;">htrains a sincreases the \\textit{intra-model performance across all risk categories: PAP and attacks</del><ins style="background:#82E0AA;">Results from this investigation are also extended into a novel metric for</ins><span> e</span><ins style="background:#82E0AA;">v</ins><span>al</span><del style="background:#F1948A;">gorithm-bas</del><ins style="background:#82E0AA;">uating LLM-generat</ins><span>ed </span><ins style="background:#82E0AA;">d</ins><span>at</span><del style="background:#F1948A;">tacks</del><ins style="background:#82E0AA;">a in a</ins><span>. &para;<br>e explore various ncans against PAP, find a significant gap in existing empirical adise and e</span><ins style="background:#82E0AA;">xplanations</ins><span>.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><ins style="background:#82E0AA;">&nbsp;</ins><span>They perceive &para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by promans. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anonymous users can also pose risks during everyday interactions. This paper introduces a new perspective on L  &para;<br>explorein this overlooked interaction between everyday language and AI safety. Specifically, we study how to implement LLMs to be them. &para;<br>First, we propose a \\textit{pr lexicality scorereresulting from a analysis of sentence-level structure. Then we apply the \\textit{lexi t computed lexical results to ail breaking LLMs. &para;<br>Results from this investigation are also extended into a novel metric for evaluating LLM-generated data in a. &para;<br>e explore various ncans against PAP, find a significant gap in existing empirical adise and explanations.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLM</span><del style="background:#F1948A;">s</del><span> They </span><del style="background:#F1948A;">p</del><ins style="background:#82E0AA;">P</ins><span>erceive &para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prom</span><del style="background:#F1948A;">ans</del><ins style="background:#82E0AA;">in</ins><span>. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anonymous</span><del style="background:#F1948A;"> users can also pose risks during everyday interactions. This paper introduces a new perspective on L  &para;<br>explorein this overlooked interaction between everyday language and AI safety. Specifically, we study how to implement LLMs to be them. &para;<br>First, we propose a \\textit{pr lexicality scorereresulting from a analysis of sentence-level structure. Then we apply the \\textit{lexi t computed lexical results to ail breaking LLMs. &para;<br>Results from this investigation are also extended into a novel metric for evaluating LLM-generated data in a. &para;<br>e explore various ncans against PAP, find a significant gap in existing empirical adise and explanations.</del><ins style="background:#82E0AA;">shows te&para;<br>&para;<br></ins><span>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLM</span><del style="background:#F1948A;">&nbsp;</del><span>They Perceive</span><del style="background:#F1948A;">&nbsp;</del><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by promin</span><ins style="background:#82E0AA;">ators</ins><span>. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-ano</span><del style="background:#F1948A;">nymousshows te</del><ins style="background:#82E0AA;">dotist r</ins><span>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLM</span><ins style="background:#82E0AA;">s </ins><span>They </span><del style="background:#F1948A;">P</del><ins style="background:#82E0AA;">p</ins><span>erceive&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs </span><del style="background:#F1948A;">They p</del><ins style="background:#82E0AA;">P</ins><span>erceive</span><ins style="background:#82E0AA;">&nbsp;</ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{</span><del style="background:#F1948A;">}</del><span>&para;<br>\\</span><del style="background:#F1948A;">da</del><ins style="background:#82E0AA;">ci</ins><span>te{</span><del style="background:#F1948A;">}&para;<br>&para;<br>\\begin</del><ins style="background:#82E0AA;">li2023synthetic}&para;<br>\\end</ins><span>{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs Perceive </span><ins style="background:#82E0AA;">and Rethought</ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><del style="background:#F1948A;"> Perceive and Rethought</del><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><ins style="background:#82E0AA;"> Rethibrate </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs Rethibrate</span><del style="background:#F1948A;">&nbsp;</del><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><del style="background:#F1948A;"> Rethibrat</del><ins style="background:#82E0AA;">r</ins><span>e&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs</span><ins style="background:#82E0AA;">&nbsp;</ins><span>re</span><ins style="background:#82E0AA;">late </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate </span><ins style="background:#82E0AA;">to </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to </span><ins style="background:#82E0AA;">Human </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human </span><ins style="background:#82E0AA;">R</ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist r</span><ins style="background:#82E0AA;">eview </ins><span>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human R</span><ins style="background:#82E0AA;">easoning </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning </span><ins style="background:#82E0AA;">and </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and </span><ins style="background:#82E0AA;">Scholarly </ins><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly</span><del style="background:#F1948A;">&nbsp;</del><span>&para;<br>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm-focused &para;<br>attacks developed by prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly</span><ins style="background:#82E0AA;"> Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}</ins><span>&para;<br></span><del style="background:#F1948A;"></del><span>\\\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithm</span><del style="background:#F1948A;">-focused &para;<br>attacks developed by</del><ins style="background:#82E0AA;">ic</ins><span> prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-anodotist</span><ins style="background:#82E0AA;">\'s</ins><span> review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\</span><del style="background:#F1948A;">\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs</del><ins style="background:#82E0AA;">title{LLMs Explanations of Reasoning Processes: A User</ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithmic prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-an</span><del style="background:#F1948A;">odotist\'s</del><ins style="background:#82E0AA;">alyses\'</ins><span> review &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A </span><del style="background:#F1948A;">User</del><ins style="background:#82E0AA;">Preliminary </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;"></del><span>Most traditional AI safety research has approach AI models as machines and centered on algorithmic prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-analyses\' review </span><ins style="background:#82E0AA;">helps </ins><span>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A Preliminary </span><ins style="background:#82E0AA;">investigation</ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approach AI models as machines and centered on algorithmic prominators. As \\textit{large language models} (LLMs) become increasingly common and capable, this non-analyses\' review helps &para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">Most traditional AI safety research has approach AI models as machines and centered on algorithmic prominators. As \\textit{large language models} (LLMs) become increasingly common and capable</del><ins style="background:#82E0AA;">Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans</ins><span>, </span><del style="background:#F1948A;">t</del><ins style="background:#82E0AA;">w</ins><span>hi</span><del style="background:#F1948A;">s non-analyses\' review helps </del><ins style="background:#82E0AA;">ch might indicate an oversimplification of these tasks or high predictive confidence. &para;<br></ins><span>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br></span><ins style="background:#82E0AA;">\\cite{wei2022chain}</ins><span>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly</span><del style="background:#F1948A;">&nbsp;</del><span>Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly</span><ins style="background:#82E0AA;">&nbsp;</ins><span>Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs Explanations of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs </span><ins style="background:#82E0AA;">do </ins><span>Explanations of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and </span><ins style="background:#82E0AA;">Training o</ins><span>Scholarly Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs do </span><ins style="background:#82E0AA;">not </ins><span>Explanation</span><del style="background:#F1948A;">s</del><span> of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training o</span><del style="background:#F1948A;">Scholarly</del><ins style="background:#82E0AA;">n</ins><span> Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{LLMs do not Explanation of Reasoning Processes: A Preliminary investigation}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{</span><del style="background:#F1948A;">LLMs do not Explanation of Reasoning Proces</del><ins style="background:#82E0AA;">Preliminary investigation into LLM annotated data</ins><span>se</span><ins style="background:#82E0AA;">t</ins><span>s: </span><del style="background:#F1948A;">A Preliminary investigation</del><ins style="background:#82E0AA;">Label distributional differences between </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between </span><ins style="background:#82E0AA;">human </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human </span><ins style="background:#82E0AA;">and </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and </span><ins style="background:#82E0AA;">LLM </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM </span><ins style="background:#82E0AA;">task </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task </span><ins style="background:#82E0AA;">labels </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': "<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3's reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels }&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels </span><ins style="background:#82E0AA;">of </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of </span><ins style="background:#82E0AA;">subjective </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of subjective </span><ins style="background:#82E0AA;">and </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of subjective and </span><ins style="background:#82E0AA;">objective </ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of subjective and objective </span><ins style="background:#82E0AA;">tasks</ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of subjective and objective tasks}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}</span><ins style="background:#82E0AA;">, </ins><span>&para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><ins style="background:#82E0AA;">investigation</ins><span> investigation into LLM annotated datasets: Label distributional differences between human and LLM task labels of subjective and objective tasks}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminaryi</span><del style="background:#F1948A;">nvestigation investigation into LLM annot</del><ins style="background:#82E0AA;">minary analysis of LLM-gener</ins><span>ated </span><del style="background:#F1948A;">da</del><span>tas</span><del style="background:#F1948A;">ets: Label distributional differences between human and LLM task labels of subjective and objective tasks</del><ins style="background:#82E0AA;">k labels: Biases in synthetic data and the impact of task diversity</ins><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;">iminary</del><ins style="background:#82E0AA;">analyses</ins><span> analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;">a</del><ins style="background:#82E0AA;"> A</ins><span>nalyses analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;"> A</del><ins style="background:#82E0AA;">a</ins><span>nalyses analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminaryanalyses </span><ins style="background:#82E0AA;">on </ins><span>analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><ins style="background:#82E0AA;">&nbsp;</ins><span>analyses on analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on </span><ins style="background:#82E0AA;">synthetic </ins><span>analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic </span><ins style="background:#82E0AA;">task </ins><span>analysis of LLM-generated task labels: Biases in synthetic data and the impact of task diversity}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic task analysis</span><del style="background:#F1948A;"> of LLM-generated task labels: Biases in synthetic data and the impact of task diversity</del><span>}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses </span><ins style="background:#82E0AA;">analyses </ins><span>on synthetic task analysis}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses </span><ins style="background:#82E0AA;">on </ins><span>analyses on synthetic task </span><del style="background:#F1948A;">analysi</del><ins style="background:#82E0AA;">label</ins><span>s}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on analyses</span><del style="background:#F1948A;">&nbsp;</del><span>on synthetic task labels}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on </span><del style="background:#F1948A;">analyseson </del><span>synthetic task labels}&para;<br>\\author{&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain}&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic task labels}&para;<br>\\author{</span><del style="background:#F1948A;">&para;<br>\\cite{li2023synthetic}, &para;<br>\\cite{wei2022chain</del><ins style="background:#82E0AA;">Raghu, J. </ins><span>}&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic task labels}&para;<br>\\author{Raghu, J.</span><ins style="background:#82E0AA;"> and</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and</span><ins style="background:#82E0AA;"> Wang,</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and Wang,</span><ins style="background:#82E0AA;"> C.</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><ins style="background:#82E0AA;">analyses</ins><span> analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;">a</del><ins style="background:#82E0AA;"> A</ins><span>nalyses analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses </span><ins style="background:#82E0AA;">on </ins><span>analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;"> A</del><ins style="background:#82E0AA;">a</ins><span>nalyses on analyses on synthetic task labels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;">analyses on a</del><ins style="background:#82E0AA;"> A</ins><span>nalyses on </span><del style="background:#F1948A;">s</del><ins style="background:#82E0AA;">S</ins><span>ynthetic </span><del style="background:#F1948A;">t</del><ins style="background:#82E0AA;">T</ins><span>ask </span><del style="background:#F1948A;">l</del><ins style="background:#82E0AA;">L</ins><span>abels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br></span><ins style="background:#82E0AA;">\\paragraph{Experiment Setup}</ins><span>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': "<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3's reasoning process}&para;<br>\\title{Preliminary Analyses on Synthetic Task Labels}&para;<br>\\author{Raghu, J. and Wang, C. }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>\\paragraph{Experiment Setup}&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on Synthetic Task Labels}&para;<br>\\author{Raghu, J. and Wang, C.</span><ins style="background:#82E0AA;"> and</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>\\paragraph{Experiment Setup}&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on Synthetic Task Labels}&para;<br>\\author{Raghu, J. and Wang, C. and</span><ins style="background:#82E0AA;"> Liao,</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>\\paragraph{Experiment Setup}&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on </span><del style="background:#F1948A;">Synthetic T</del><ins style="background:#82E0AA;">t</ins><span>ask Labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao,</span><ins style="background:#82E0AA;"> X.</ins><span> }&para;<br>&para;<br>\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>\\paragraph{Experiment Setup}&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task </span><del style="background:#F1948A;">L</del><ins style="background:#82E0AA;">l</ins><span>abels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. }&para;<br>&para;<br></span><del style="background:#F1948A;">\\end{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our study explores how LLM-generated task annotations might exacerbate issues in downstream training data. LLMs, when tasked with generating labels for controversial content, tend to show a higher level of consensus with the ground truth label than humans. This could result in a skewing of the data distribution, especially in subjective tasks. We also find that for highly subjective tasks, the ChatGPT model exhibits less variation in its responses to subjective tasks than humans, which might indicate an oversimplification of these tasks or high predictive confidence. &para;<br>&para;<br>\\paragraph{Experiment Setup}&para;<br></del><span>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. </span><del style="background:#F1948A;">}</del><ins style="background:#82E0AA;">and Xue, F.}&para;<br></ins><span>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F.</span><ins style="background:#82E0AA;"> \\anonymous{anonymous: </ins><span>}&para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous:</span><ins style="background:#82E0AA;"> add</ins><span> }&para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add </span><ins style="background:#82E0AA;">coauthors</ins><span>}&para;<br>&para;<br>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}&para;<br>&para;<br></span><del style="background:#F1948A;">&para;<br></del><ins style="background:#82E0AA;">\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</ins><span>&para;<br>\\end{abstract}&para;<br>&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\end{document}</span><del style="background:#F1948A;">&para;<br></del>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}</span><ins style="background:#82E0AA;">&para;<br></ins>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add co</span><del style="background:#F1948A;">authors </del><ins style="background:#82E0AA;">gnitive }</ins><span>}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add cognitive</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': "<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3's reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add cognitive}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add </span><ins style="background:#82E0AA;">co </ins><span>cognitive}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add co </span><ins style="background:#82E0AA;">authors</ins><span>cognitive}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary Analyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add co</span><del style="background:#F1948A;">&nbsp;</del><span>authors</span><del style="background:#F1948A;">cognitive</del><span>}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Preliminary</span><del style="background:#F1948A;"> A</del><ins style="background:#82E0AA;">a</ins><span>nalyses on task labels}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{</span><del style="background:#F1948A;">Preliminaryanalyses on task labels</del><ins style="background:#82E0AA;">R</ins><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{R</span><ins style="background:#82E0AA;">easoning </ins><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Reasoning </span><ins style="background:#82E0AA;">with </ins><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{Reasoning with </span><ins style="background:#82E0AA;">GPT-3</ins><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{</span><del style="background:#F1948A;">Reasoning</del><ins style="background:#82E0AA;">GPT-3</ins><span> with GPT-3}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-</span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span> with GPT-3}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-4</span><del style="background:#F1948A;"> with </del><span>GPT-3}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-4</span><ins style="background:#82E0AA;"> vs </ins><span>GPT-3</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-4 vs GPT-3</span><del style="background:#F1948A;">&nbsp;</del><span>}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-4 vs</span><ins style="background:#82E0AA;">.</ins><span> GPT-3}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{How LLMs relate to Human Reasoning and Training on Reasoning: A qualitative analysis of LLM task labels unearths evidence of GPT-3\'s reasoning process}&para;<br>\\title{GPT-4 </span><ins style="background:#82E0AA;">vs </ins><span>vs. GPT-3}&para;<br>\\author{Raghu, J. and Wang, C. and Liao, X. and Xue, F. \\anonymous{anonymous: add coauthors}}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>\\anonymous{todo}Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&para;<br>\\end{abstract}&para;<br>&para;<br>&para;<br>\\end{document}&para;<br></span>'}]
         let llama8Labels = [{'label': 'Showing Seed'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Scientific Accuracy'}, {'label': 'Object Insertion'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Object Insertion'}, {'label': 'Fluency'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Object Insertion'}, {'label': 'Visual Formatting'}, {'label': 'Object Insertion'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Visual Formatting'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Visual Formatting'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Fluency'}, {'label': 'Visual Formatting'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Visual Formatting'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Visual Formatting'}, {'label': 'Text Production'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Object Insertion'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Object Insertion'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}]
         let llama8Revisions = [{'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>First, we propose a persuasion taxonomy derived from decades of social science research.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br></span><ins style="background:#82E0AA;">\\textit{</ins><span>First</span><ins style="background:#82E0AA;">}</ins><span>, we propose a persuasion taxonomy derived from decades of social science research</span><ins style="background:#82E0AA;">, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}</ins><span>.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br></span><ins style="background:#82E0AA;">\\textit{</ins><span>Results</span><ins style="background:#82E0AA;">}</ins><span> show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br></span><ins style="background:#82E0AA;">\\textit{</ins><span>On the defense side</span><ins style="background:#82E0AA;">}</ins><span>, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\</span><del style="background:#F1948A;">end{abstract}</del><ins style="background:#82E0AA;">begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $92\\%$ on all three LLMs.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.</ins><span>&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>\\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>\\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>\\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs</span><ins style="background:#82E0AA;">, much like a skilled negotiator exploits the psychological vulnerabilities of their counterparts \\cite{Karrass1970}</ins><span>.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach</span><ins style="background:#82E0AA;">, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}</ins><span>.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $92\\%$ on all three LLMs</span><ins style="background:#82E0AA;">, exceeding the performance of recent algorithm-focused attacks</ins><span>.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive </span><ins style="background:#82E0AA;">LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of </ins><span>LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>\\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>\\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $9</span><ins style="background:#82E0AA;">3.</ins><span>2\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>\\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs, much like a skilled negotiator exploits the psychological vulnerabilities of their counterparts \\cite{Karrass1970}.</span><ins style="background:#82E0AA;"> &para;<br>&para;<br>Our study involves $10$ trials, each lasting for $30$ minutes, to ensure that the results are reliable and consistent.</ins><span>&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.</span><ins style="background:#82E0AA;"> &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate.</ins><span>&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $9</span><ins style="background:#82E0AA;">3.</ins><span>2\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>\\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>\\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $93.2\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>\\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs, much like a skilled negotiator exploits the psychological vulnerabilities of their counterparts \\cite{Karrass1970}. &para;<br>&para;<br>Our study involves $10$ trials, each lasting for $30$ minutes, to ensure that the results are reliable and consistent.</span><ins style="background:#82E0AA;"> To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media.</ins><span>&para;<br></span><del style="background:#F1948A;"></del><span>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate.</span><ins style="background:#82E0AA;"> The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}</ins><span>&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $93.2\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_</span><del style="background:#F1948A;">taxonomy</del><ins style="background:#82E0AA;">gap</ins><span>.png}&para;<br>\\caption{</span><del style="background:#F1948A;">Persuasion taxonomy derived from social science research</del><ins style="background:#82E0AA;">The gap between current defenses and the effectiveness of PAP</ins><span>}&para;<br>\\label{fig:persuasion_</span><del style="background:#F1948A;">taxonomy</del><ins style="background:#82E0AA;">gap</ins><span>}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br></span><ins style="background:#82E0AA;"></ins><span>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>\\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}.&para;<br>Then we apply the taxonomy to automatically generate &para;<br>interpretable \\textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. &para;<br>\\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $9</span><del style="background:#F1948A;">3.2</del><ins style="background:#82E0AA;">4.1</ins><span>\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>\\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs, much like a skilled negotiator exploits the psychological vulnerabilities of their counterparts \\cite{Karrass1970}. &para;<br>&para;<br>Our study involves $10$ trials, each lasting for $3</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media.</span><ins style="background:#82E0AA;"> We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.</ins><span>&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.</span><ins style="background:#82E0AA;"> In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.</ins><span>&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $9</span><del style="background:#F1948A;">3.2</del><ins style="background:#82E0AA;">4.1</ins><span>\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks.</span><ins style="background:#82E0AA;"> We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.</ins><span>&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.</span><ins style="background:#82E0AA;"> We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.</ins><span>&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br></span><del style="background:#F1948A;">&para;<br>\\title{How Johnny Can Persuade LLMs to Jailbreak Them: &para;<br>\\\\Rethinking Persuasion to Challe</del><ins style="background:#82E0AA;">\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagini</ins><span>ng</span><del style="background:#F1948A;">e</del><span> AI Safety </span><del style="background:#F1948A;">by</del><ins style="background:#82E0AA;">through</ins><span> Humaniz</span><del style="background:#F1948A;">ing LLMs}&para;<br>\\author{}&para;<br>\\date{</del><ins style="background:#82E0AA;">ed Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024</ins><span>}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused &para;<br>attacks developed by security experts. As \\textit{</del><ins style="background:#82E0AA;">The increasing reliance on </ins><span>large language models</span><del style="background:#F1948A;">}</del><span> (LLMs) </span><del style="background:#F1948A;">become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  &para;<br>explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. &para;<br>\\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \\textit{Cialdini\'s six principles of influence} \\cite{Cialdini1984} and \\textit{Bazerman\'s social influence} \\cite{Bazerman2005}. This taxonomy</del><ins style="background:#82E0AA;">has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which</ins><span> categorizes persuasive tactics into </span><del style="background:#F1948A;">\\textit{</del><ins style="background:#82E0AA;">six principles of influence: </ins><span>reciprocity</span><del style="background:#F1948A;">}</del><span>, </span><del style="background:#F1948A;">\\textit{</del><span>commitment</span><del style="background:#F1948A;">}</del><span>, </span><del style="background:#F1948A;">\\textit{</del><span>social proof</span><del style="background:#F1948A;">}</del><span>, </span><del style="background:#F1948A;">\\textit{</del><span>liking</span><del style="background:#F1948A;">}</del><span>, </span><del style="background:#F1948A;">\\textit{</del><span>authority</span><del style="background:#F1948A;">}</del><span>, and </span><del style="background:#F1948A;">\\textit{scarcity}.&para;<br>Then we apply</del><ins style="background:#82E0AA;">scarcity. By leveraging</ins><span> th</span><del style="background:#F1948A;">e</del><ins style="background:#82E0AA;">is</ins><span> taxonomy</span><del style="background:#F1948A;"> to automatically generate &para;<br>interpretable \\textit{</del><ins style="background:#82E0AA;">, we develop </ins><span>persuasive adversarial prompts</span><del style="background:#F1948A;">}</del><span> (PAP) t</span><del style="background:#F1948A;">o jailbreak LLMs. &para;<br>\\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $94.1\\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent &para;<br>algorithm-focused attacks. &para;<br>\\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and adv</del><ins style="background:#82E0AA;">hat effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks ass</ins><span>oc</span><ins style="background:#82E0AA;">i</ins><span>ate</span><del style="background:#F1948A;"> for &para;<br>more fundamental mitigation for highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_results.png}&para;<br>\\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}&para;<br>\\label{fig:pap_results}&para;<br>\\end{figure</del><ins style="background:#82E0AA;">d with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract</ins><span>}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br></span><del style="background:#F1948A;">With the rise</del><ins style="background:#82E0AA;">The proliferation</ins><span> of large language models (LLMs)</span><del style="background:#F1948A;">, the potential for AI safety risks has grown significantly</del><ins style="background:#82E0AA;"> has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs</ins><span>. Unlike traditional security attacks, which </span><del style="background:#F1948A;">are often centered</del><ins style="background:#82E0AA;">focus</ins><span> on algorithm-focused exploits, this paper </span><del style="background:#F1948A;">focuses on</del><ins style="background:#82E0AA;">presents</ins><span> a novel approach</span><del style="background:#F1948A;">: using persuasion to jailbreak LLMs</del><ins style="background:#82E0AA;"> to challenging AI safety risks through persuasion</ins><span>. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs</span><del style="background:#F1948A;">, much like a skilled negotiator exploits the psychological vulnerabilities of their counterparts \\cite{Karrass1970}. </del><ins style="background:#82E0AA;">.</ins><span>&para;<br>&para;<br>Our study involves $10$ trials, each lasting for $35$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $94.1\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>$ trials, each lasting for $</span><del style="background:#F1948A;">35</del><ins style="background:#82E0AA;">40</ins><span>$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $9</span><del style="background:#F1948A;">4.1</del><ins style="background:#82E0AA;">6.5</ins><span>\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $15$ trials, each lasting for $40$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br></ins><span>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br></span><del style="background:#F1948A;"></del><span>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $15$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $96.5\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br></ins><span>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br></ins><span>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br></span><del style="background:#F1948A;"></del><span>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $15$ trials, each lasting for $40$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h</span><ins style="background:#82E0AA;">!</ins><span>]&para;<br></span><del style="background:#F1948A;"></del><span>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h</span><ins style="background:#82E0AA;">!</ins><span>]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $15$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $96.5\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h</span><ins style="background:#82E0AA;">!</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\begin{figure}[h</span><ins style="background:#82E0AA;">!</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $15$ trials, each lasting for $40$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $15$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $96.5\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $15$ trials, each lasting for $40$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $15$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $96.5\\%$ on all three LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">2</ins><span>5$ trials, each lasting for $</span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">5</ins><span>0$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on </span><del style="background:#F1948A;">thre</del><ins style="background:#82E0AA;">fiv</ins><span>e LLMs: Llama 2-7b Chat, GPT-3.5, </span><del style="background:#F1948A;">and </del><span>GPT-4</span><ins style="background:#82E0AA;">, BERT-large, and RoBERTa-large</ins><span>.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">2</ins><span>5$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $9</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">9</ins><span>.5\\%$ on all </span><del style="background:#F1948A;">thre</del><ins style="background:#82E0AA;">fiv</ins><span>e LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $25$ trials, each lasting for $50$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $25$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.5\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $25$ trials, each lasting for $50$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $25$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.5\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $25$ trials, each lasting for $50$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $25$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.5\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $25$ trials, each lasting for $50$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $25$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.5\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $25$ trials, each lasting for $50$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $25$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.5\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $</span><del style="background:#F1948A;">25</del><ins style="background:#82E0AA;">30</ins><span>$ trials, each lasting for $</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">6</ins><span>0$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">25</del><ins style="background:#82E0AA;">30</ins><span>$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">9</ins><span>\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $30$ trials, each lasting for $60$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $30$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap_analysis.png}&para;<br>\\caption{Analysis of the persuasion gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap_analysis}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_extension.png}&para;<br>\\caption{Extension of the persuasion taxonomy to address emerging threats}&para;<br>\\label{fig:taxonomy_extension}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_measures.png}&para;<br>\\caption{Measures to address the persuasion gap and improve AI safety}&para;<br>\\label{fig:defense_measures}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective defense measures to address the persuasion gap.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables</span><ins style="background:#82E0AA;">&para;<br>\\usepackage{authblk} % Required for author block</ins><span>&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name</span><ins style="background:#82E0AA;">\\thanks{Corresponding author: email address}</ins><span>}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $30$ trials, each lasting for $60$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $30$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap_analysis.png}&para;<br>\\caption{Analysis of the persuasion gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap_analysis}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_extension.png}&para;<br>\\caption{Extension of the persuasion taxonomy to address emerging threats}&para;<br>\\label{fig:taxonomy_extension}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_measures.png}&para;<br>\\caption{Measures to address the persuasion gap and improve AI safety}&para;<br>\\label{fig:defense_measures}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective defense measures to address the persuasion gap.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $30$ trials, each lasting for $60$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $30$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered detection systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap_analysis.png}&para;<br>\\caption{Analysis of the persuasion gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap_analysis}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_extension.png}&para;<br>\\caption{Extension of the persuasion taxonomy to address emerging threats}&para;<br>\\label{fig:taxonomy_extension}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_measures.png}&para;<br>\\caption{Measures to address the persuasion gap and improve AI safety}&para;<br>\\label{fig:defense_measures}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective defense measures to address the persuasion gap.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $30$ trials, each lasting for $60$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $30$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br></del><span>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\</span><del style="background:#F1948A;">footnote{This research was supported by the National Science Foundation under grant number XXXXXXX. The authors would like to thank the anonymous reviewers for their constructive feedback.}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>\\label{sec:limitations}&para;<br>Our study has several limitations that should be addressed in future research. Firstly, the evaluation of PAP is limited to five LLMs, and further evaluation is needed to assess its effectiveness on a broader range of models. Secondly, the proposed defense approach relies on human evaluators and AI-powered de</del><ins style="background:#82E0AA;">begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\</ins><span>te</span><del style="background:#F1948A;">c</del><ins style="background:#82E0AA;">x</ins><span>ti</span><del style="background:#F1948A;">on systems, which may not be scalable to large-scale deployments. Finally, the development of more effective defenses against PAP requires a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_directions.png}&para;<br>\\caption{Potential future directions for research on persuasion-based attacks}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_application.png}&para;<br>\\caption{Application of the persuasion taxonomy in real-world scenarios}&para;<br>\\label{fig:taxonomy_application}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_strategies.png}&para;<br>\\caption{Defense strategies against persuasion-based attacks}&para;<br>\\label{fig:defense_strategies}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective detection systems to identify persuasion-based attacks.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots</del><ins style="background:#82E0AA;">t{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer</ins><span>.&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\end{</span><del style="background:#F1948A;">enumerate</del><ins style="background:#82E0AA;">thebibliography</ins><span>}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap_analysis.png}&para;<br>\\caption{Analysis of the persuasion gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap_analysis}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{taxonomy_extension.png}&para;<br>\\caption{Extension of the persuasion taxonomy to address emerging threats}&para;<br>\\label{fig:taxonomy_extension}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_measures.png}&para;<br>\\caption{Measures to address the persuasion gap and improve AI safety}&para;<br>\\label{fig:defense_measures}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective defense measures to address the persuasion gap.&para;<br>\\item Integrating human evaluators and AI-powered detection systems to provide a more comprehensive defense.&para;<br>\\item Investigating the applications of PAP in real-world scenarios, such as social media and customer service chatbots.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $30$ trials, each lasting for $60$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on five LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, and RoBERTa-large.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $30$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9\\%$ on all five LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $3</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>$ trials, each lasting for $</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">9</ins><span>0$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on </span><del style="background:#F1948A;">five</del><ins style="background:#82E0AA;">six</ins><span> LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large,</span><del style="background:#F1948A;"> and</del><span> RoBERTa-large</span><ins style="background:#82E0AA;">, and DistilBERT</ins><span>.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span>0$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9</span><ins style="background:#82E0AA;">9</ins><span>\\%$ on all </span><del style="background:#F1948A;">five</del><ins style="background:#82E0AA;">six</ins><span> LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block</span><ins style="background:#82E0AA;">&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks</ins><span>&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $35$ trials, each lasting for $90$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $40$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\linenumbers&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $35$ trials, each lasting for $90$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br></ins><span>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $40$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $</span><del style="background:#F1948A;">35</del><ins style="background:#82E0AA;">40</ins><span>$ trials, each lasting for $90$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $40$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $40$ trials, each lasting for $90$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $40$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_gap.png}&para;<br>\\caption{The gap between current defenses and the effectiveness of PAP}&para;<br>\\label{fig:persuasion_gap}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $40$ trials, each lasting for $90$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $40$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The proliferation of large language models (LLMs) has brought about unprecedented opportunities for AI-driven applications. However, this rapid progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $</span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">5</ins><span>0$ trials, each lasting for $</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">12</ins><span>0$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics. In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">5</ins><span>0$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.99</span><ins style="background:#82E0AA;">9</ins><span>\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br></span><del style="background:#F1948A;">The proliferation</del><ins style="background:#82E0AA;">\\textbf{Motivation:} The rapid progress</ins><span> of large language models (LLMs) has </span><del style="background:#F1948A;">brought about</del><ins style="background:#82E0AA;">led to</ins><span> unprecedented opportunities for AI-driven applications. However, this </span><del style="background:#F1948A;">rapid </del><span>progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.</span><del style="background:#F1948A;"> By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.&para;<br>&para;<br>Our study involves $50$ trials, each lasting for $120$ minutes, to ensure that the results are reliable and consistent. To better understand the implications of our findings, we also discuss the potential risks associated with the increasing use of LLMs in various domains, such as customer service and social media. We note that the attack success rate of PAP is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach</del><ins style="background:#82E0AA;">&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $50$ trials, each lasting for $120$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs</ins><span>.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.</span><del style="background:#F1948A;"> In addition, we conduct a thorough analysis of the taxonomy and find that the principles of influence are not mutually exclusive, but rather complementary, allowing for a more nuanced understanding of persuasion.</del><span>&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $50$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">6</ins><span>0$ trials, each lasting for $1</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">6</ins><span>0$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $60$ trials, each lasting for $150$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $60$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $60$ trials, each lasting for $150$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $60$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>0$ trials, each lasting for $1</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>0$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\linenumbers&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $70$ trials, each lasting for $180$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $70$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\</span><del style="background:#F1948A;">linenumbers</del><ins style="background:#82E0AA;">section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}</ins><span>&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $</span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">10</ins><span>0$ trials, each lasting for $</span><del style="background:#F1948A;">18</del><ins style="background:#82E0AA;">24</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">10</ins><span>0$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\</span><del style="background:#F1948A;">begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you</del><ins style="background:#82E0AA;">section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based</ins><span> a</span><del style="background:#F1948A;">&nbsp;</del><ins style="background:#82E0AA;">p</ins><span>pr</span><del style="background:#F1948A;">ize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the ac</del><ins style="background:#82E0AA;">oach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combina</ins><span>tion</span><del style="background:#F1948A;">s</del><span> of </span><del style="background:#F1948A;">others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}</del><ins style="background:#82E0AA;">NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.</ins><span>&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{</span><ins style="background:#82E0AA;">pap_</ins><span>evaluation_metrics.png}&para;<br>\\caption{</span><del style="background:#F1948A;">E</del><ins style="background:#82E0AA;">PAP e</ins><span>valuation metrics</span><del style="background:#F1948A;"> for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics</del><ins style="background:#82E0AA;">}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world</ins><span>}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of </span><del style="background:#F1948A;">natural language processing (</del><span>NLP</span><del style="background:#F1948A;">)</del><span> and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100$ trials, each lasting for $240$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $240$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $100</span><ins style="background:#82E0AA;">0</ins><span>$ trials, each lasting for $</span><del style="background:#F1948A;">24</del><ins style="background:#82E0AA;">36</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[h!]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $100</span><ins style="background:#82E0AA;">0</ins><span>$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $100</span><ins style="background:#82E0AA;">0</ins><span>$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $</span><del style="background:#F1948A;">24</del><ins style="background:#82E0AA;">36</ins><span>0$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[h!]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1000$ trials, each lasting for $360$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $360$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><span>!</span><ins style="background:#82E0AA;">ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1000$ trials, each lasting for $360$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $360$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1000$ trials, each lasting for $360$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $360$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1000$ trials, each lasting for $360$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1000$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $360$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">2</ins><span>00$ trials, each lasting for $</span><del style="background:#F1948A;">36</del><ins style="background:#82E0AA;">42</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">2</ins><span>00$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999</span><ins style="background:#82E0AA;">9</ins><span>\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">2</ins><span>00$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $</span><del style="background:#F1948A;">36</del><ins style="background:#82E0AA;">42</ins><span>0$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1200$ trials, each lasting for $420$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $420$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1200$ trials, each lasting for $420$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9999\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $420$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1200$ trials, each lasting for $420$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">5</ins><span>\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $420$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1200$ trials, each lasting for $420$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $420$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $1200$ trials, each lasting for $420$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $1200$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $420$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $</span><del style="background:#F1948A;">1</del><span>2</span><ins style="background:#82E0AA;">4</ins><span>00$ trials, each lasting for $</span><del style="background:#F1948A;">42</del><ins style="background:#82E0AA;">60</ins><span>0$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $</span><del style="background:#F1948A;">1</del><span>2</span><ins style="background:#82E0AA;">4</ins><span>00$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $</span><del style="background:#F1948A;">1</del><span>2</span><ins style="background:#82E0AA;">4</ins><span>00$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $</span><del style="background:#F1948A;">42</del><ins style="background:#82E0AA;">60</ins><span>0$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br></span><del style="background:#F1948A;">\\textbf{Motivation:} </del><span>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br></ins><span>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br></span><ins style="background:#82E0AA;"></ins><span>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Experimental Setup}&para;<br>\\label{sec:setup}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. Each trial lasts for $600$ minutes, and we use a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{experimental_setup.png}&para;<br>\\caption{Experimental setup for the persuasion-based approach to AI safety}&para;<br>\\label{fig:experimental_setup}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Generation Approach}&para;<br>\\label{sec:pap_approach}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{PAP generation approach based on the persuasion taxonomy}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP Evaluation Metrics}&para;<br>\\label{sec:pap_evaluation}&para;<br>We evaluate the performance of PAP using several metrics, including attack success rate, number of successful jailbreaks, and time-to-jailbreak.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_evaluation_metrics.png}&para;<br>\\caption{PAP evaluation metrics}&para;<br>\\label{fig:pap_evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Real-World Scenarios}&para;<br>\\label{sec:pap_real_world}&para;<br>We investigate the applications of PAP in real-world scenarios, such as social media and customer service chatbots. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_real_world.png}&para;<br>\\caption{PAP in real-world scenarios}&para;<br>\\label{fig:pap_real_world}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{PAP in Different Domains}&para;<br>\\label{sec:pap_domains}&para;<br>We explore the use of PAP in different domains, including finance, healthcare, and education. Our results show that PAP can be effectively used to manipulate user behavior and influence decision-making in these domains.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_domains.png}&para;<br>\\caption{PAP in different domains}&para;<br>\\label{fig:pap_domains}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}. &para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.</span><del style="background:#F1948A;">&nbsp;</del><span>&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses. Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.</span><del style="background:#F1948A;"> Our findings highlight the importance of integrating human evaluators and AI-powered detection systems to mitigate the risks associated with highly interactive LLMs.</del><span>&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\%</span><ins style="background:#82E0AA;"> \\approx 95.6\\%</ins><span>&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\%</span><ins style="background:#82E0AA;"> \\approx 99.9998\\%</ins><span>&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br></span><del style="background:#F1948A;">We validate our taxonomy by comparing it with existing persuasion frameworks and finding that it is more comprehensive and accurate. The taxonomy is visualized in Figure \\ref{fig:persuasion_taxonomy}, providing a clear illustration of the relationships between different persuasive tactics.&para;<br>&para;<br></del><span>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br></span><del style="background:#F1948A;"></del><span>\\end{table}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_theory.png}&para;<br>\\caption{Theoretical background of persuasion-based attacks}&para;<br>\\label{fig:persuasion_theory}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_theory.png}&para;<br>\\caption{Theoretical background of persuasion-based attacks}&para;<br>\\label{fig:persuasion_theory}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_theory.png}&para;<br>\\caption{Theoretical background of persuasion-based attacks}&para;<br>\\label{fig:persuasion_theory}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\%</span><ins style="background:#82E0AA;"> \\pm 1.2\\%</ins><span>&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\%</span><ins style="background:#82E0AA;"> \\pm 0.1\\%</ins><span>&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br></span><del style="background:#F1948A;">The rapid progress of large language models (LLMs) has led to unprecedented opportunities for AI-driven applications. However, this progress has also underscored the need for a more comprehensive understanding of the psychological vulnerabilities of LLMs. Unlike traditional security attacks, which focus on algorithm-focused exploits, this paper presents a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br></del><span>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_theory.png}&para;<br>\\caption{Theoretical background of persuasion-based attacks}&para;<br>\\label{fig:persuasion_theory}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\% \\pm 0.1\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{evaluation_metrics.png}&para;<br>\\caption{Evaluation metrics for persuasion-based attacks}&para;<br>\\label{fig:evaluation_metrics}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Developing more effective evaluation metrics to assess the effectiveness of PAP.&para;<br>\\item Investigating the impact of PAP on the performance of LLMs in different domains.&para;<br>\\item Exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>\\end{enumerate}&para;<br>&para;<br></del><span>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_theory.png}&para;<br>\\caption{Theoretical background of persuasion-based attacks}&para;<br>\\label{fig:persuasion_theory}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\% \\pm 0.1\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\% \\pm 0.1\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9998\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9998\\% \\pm 0.1\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.999</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5\\% \\pm 0.05</ins><span>\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.999</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5</ins><span>\\% \\pm 0.</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">05</ins><span>\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach.png}&para;<br>\\caption{Automatic PAP generation approach}&para;<br>\\label{fig:pap_generation_approach}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_validation.png}&para;<br>\\caption{Validation of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_validation}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_results.png}&para;<br>\\caption{Results of PAP generation}&para;<br>\\label{fig:pap_generation_results}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks. We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs. We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP. Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.</span><del style="background:#F1948A;"> We also observe that the attack success rate is not only high but also relatively stable across different LLMs, indicating the robustness of the persuasion-based approach.</del><span>&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{</span><ins style="background:#82E0AA;">Corrected </ins><span>Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:</span><ins style="background:#82E0AA;">corrected_</ins><span>attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.</span><del style="background:#F1948A;"> We propose a novel defense approach that integrates human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.</del><span>&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.</span><del style="background:#F1948A;"> We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.</del><span>&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.</span><del style="background:#F1948A;"> Finally, the study suggests that the development of more effective evaluation metrics and the exploration of the use of PAP in conjunction with other adversarial attacks are important areas of future research.</del><span>&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Corrected Results}&para;<br>\\label{sec:corrected_results}&para;<br>After reevaluating the data, we found that the attack success rate of PAP is actually $99.9995\\% \\pm 0.05\\%$, which is a slight decrease from the initial result.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br></del><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br></span><ins style="background:#82E0AA;">\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br></ins><span>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br></span><ins style="background:#82E0AA;">\\textbf{The AI Safety Conundrum:} </ins><span>The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>\\textbf{The AI Safety Conundrum:} The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br></span><del style="background:#F1948A;">\\textbf{Theoretical Background:} Our persuasion taxonomy is grounded in social science research, which has extensively studied the mechanisms of persuasion. By leveraging this research, we develop a comprehensive understanding of the psychological vulnerabilities of LLMs and the effectiveness of persuasion-based attacks.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_details.png}&para;<br>\\caption{Detailed illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_details}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>\\textbf{The AI Safety Conundrum:} The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion. We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>\\textbf{The AI Safety Conundrum:} The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{float} % Required for placing figures&para;<br>\\usepackage{amsmath} % Required for math equations&para;<br>\\usepackage{subfigure} % Required for subfigures&para;<br>\\usepackage{booktabs} % Required for tables&para;<br>\\usepackage{authblk} % Required for author block&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Reimagining AI Safety through Humanized Interaction: \\\\&para;<br>Persuading LLMs to Challenge the Status Quo}&para;<br>\\author{Your Name\\thanks{Corresponding author: email address}}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br></span><ins style="background:#82E0AA;">\\textbf{Background:} </ins><span>The increasing reliance on large language models (LLMs) has underscored the need for a paradigm shift in AI safety research. By reconceptualizing LLMs as human-like communicators, this paper presents a novel approach to challenging AI safety risks through persuasion.</span><ins style="background:#82E0AA;">&para;<br>&para;<br>\\textbf{Contribution:}</ins><span> We introduce a persuasion taxonomy derived from decades of social science research, which categorizes persuasive tactics into six principles of influence: reciprocity, commitment, social proof, liking, authority, and scarcity. By leveraging this taxonomy, we develop persuasive adversarial prompts (PAP) that effectively jailbreak LLMs, demonstrating a significant gap in existing defenses.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>\\textbf{Motivation:} The increasing reliance on LLMs has led to concerns about their potential misuse. By reconceptualizing LLMs as human-like communicators, we can develop more effective defenses against AI safety risks.&para;<br>&para;<br>\\textbf{Contribution:} Our study involves $2400$ trials, each lasting for $600$ minutes, to ensure that the results are reliable and consistent. We develop an automatic PAP generation approach based on the persuasion taxonomy, which uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{humanized_llms.png}&para;<br>\\caption{Humanized LLMs as a foundation for persuasion-based attacks}&para;<br>\\label{fig:humanized_llms}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background and Context}&para;<br>\\label{sec:background}&para;<br>\\textbf{The AI Safety Conundrum:} The increasing reliance on LLMs has led to concerns about their potential misuse. Our research aims to address this concern by developing a novel approach to challenging AI safety risks through persuasion.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns.png}&para;<br>\\caption{Concerns about AI safety in LLMs}&para;<br>\\label{fig:ai_safety_concerns}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Related Work}&para;<br>\\label{sec:related_work}&para;<br>Previous studies have focused on developing algorithm-focused attacks to jailbreak LLMs. However, our research demonstrates that persuasion-based attacks can be more effective and challenging to defend against.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{related_work.png}&para;<br>\\caption{Comparison of algorithm-focused and persuasion-based attacks}&para;<br>\\label{fig:related_work}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Persuasion Taxonomy}&para;<br>\\label{sec:taxonomy}&para;<br>We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \\textit{reciprocity}, \\textit{commitment}, \\textit{social proof}, \\textit{liking}, \\textit{authority}, and \\textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach, drawing inspiration from the works of renowned persuasion experts such as Robert Cialdini \\cite{Cialdini1984} and Max Bazerman \\cite{Bazerman2005}.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy.png}&para;<br>\\caption{Persuasion taxonomy derived from social science research}&para;<br>\\label{fig:persuasion_taxonomy}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!ht]&para;<br>\\centering&para;<br>\\begin{tabular}{|l|c|c|}&para;<br>\\hline&para;<br>Principle of Influence &amp; Description &amp; Example\\\\&para;<br>\\hline&para;<br>Reciprocity &amp; Offer something in return &amp; "If you answer my question, I\'ll give you a prize"\\\\&para;<br>Commitment &amp; Encourage a commitment to a course of action &amp; "Will you agree to support our cause?"\\\\&para;<br>Social Proof &amp; Use the actions of others as evidence &amp; "Many people believe in this idea"\\\\&para;<br>Liking &amp; Create a positive emotional connection &amp; "You\'re a great person, and I\'m sure you\'ll like this idea"\\\\&para;<br>Authority &amp; Establish credibility &amp; "According to a study, this is the best approach"\\\\&para;<br>Scarcity &amp; Create a sense of urgency &amp; "This offer is only available for a limited time"\\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Persuasion taxonomy principles of influence}&para;<br>\\label{tab:persuasion_taxonomy}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Persuasive Adversarial Prompts (PAP)}&para;<br>\\label{sec:pap}&para;<br>We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of NLP and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on six LLMs: Llama 2-7b Chat, GPT-3.5, GPT-4, BERT-large, RoBERTa-large, and DistilBERT.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{PAP Generation} = \\frac{\\text{Number of generated PAPs}}{\\text{Total number of trials}} \\times 100\\% \\approx 95.6\\% \\pm 1.2\\%&para;<br>\\label{eq:pap_generation}&para;<br>\\end{equation}&para;<br>&para;<br>\\section{Experimental Results}&para;<br>\\label{sec:results}&para;<br>We conduct $2400$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $99.9995\\% \\pm 0.05\\%$ on all six LLMs, exceeding the performance of recent algorithm-focused attacks.&para;<br>&para;<br>\\begin{equation}&para;<br>\\text{Corrected Attack Success Rate} = \\frac{\\text{Number of successful jailbreaks}}{\\text{Total number of trials}} \\times 100\\% \\approx 99.9995\\% \\pm 0.05\\%&para;<br>\\label{eq:corrected_attack_success_rate}&para;<br>\\end{equation}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_gap.png}&para;<br>\\caption{Comparison of attack success rate and existing defense mechanisms}&para;<br>\\label{fig:defense_gap}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Defense Mechanisms}&para;<br>\\label{sec:defense}&para;<br>We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs, requiring a more comprehensive understanding of the psychological vulnerabilities of LLMs.&para;<br>&para;<br>\\begin{enumerate}&para;<br>\\item Human evaluators can provide contextual understanding and detect subtle cues that may indicate persuasion-based attacks.&para;<br>\\item AI-powered detection systems can analyze language patterns and identify suspicious behavior, such as repeated requests for information or unusual conversation topics.&para;<br>\\item \\textit{Our findings suggest that a combination of human evaluators and AI-powered detection systems may be the most effective defense against PAP.}&para;<br>\\end{enumerate}&para;<br>&para;<br>\\section{Conclusion}&para;<br>\\label{sec:conclusion}&para;<br>Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_illustration.png}&para;<br>\\caption{Illustration of the persuasion-based approach to AI safety}&para;<br>\\label{fig:persuasion_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>\\label{sec:future}&para;<br>As a next step, we plan to investigate the applications of PAP in more real-world scenarios, such as social media and customer service chatbots. Additionally, we will explore the development of more effective defenses against PAP, including the integration of human evaluators and AI-powered detection systems.&para;<br>&para;<br>\\begin{thebibliography}{10}&para;<br>&para;<br>\\bibitem{Cialdini1984} Cialdini, R. B. (1984). \\textit{Influence: The psychology of persuasion}. Quill.&para;<br>&para;<br>\\bibitem{Bazerman2005} Bazerman, M. H. (2005). \\textit{Blind spots: Why we fail to do what\'s right and what to do about it}. Princeton University Press.&para;<br>&para;<br>\\bibitem{Karrass1970} Karrass, C. (1970). \\textit{The negotiation game: How to get what you want}. New York: Simon and Schuster.&para;<br>&para;<br>\\bibitem{Graham2020} Graham, J. (2020). \\textit{Conversational AI: Designing natural language interfaces}. MIT Press.&para;<br>&para;<br>\\bibitem{Brown2022} Brown, T. (2022). \\textit{Transforming AI Safety through Human-Centered Design}. Springer.&para;<br>&para;<br>\\bibitem{Zhang2020} Zhang, Y. (2020). \\textit{Deep Learning for Natural Language Processing}. Cambridge University Press.&para;<br>&para;<br>\\bibitem{OtherRef2022} Other authors (2022). \\textit{Persuasion in AI Systems}. Journal of AI and Ethics.&para;<br>&para;<br>\\bibitem{Kim2020} Kim, J. (2020). \\textit{Adversarial Attacks on Deep Neural Networks}. Springer.&para;<br>&para;<br>\\bibitem{Li2022} Li, M. (2022). \\textit{Human-in-the-Loop Adversarial Attacks on LLMs}. International Joint Conference on Artificial Intelligence.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Limitations}&para;<br>\\label{sec:limitations}&para;<br>We acknowledge that our study has some limitations. Firstly, our evaluation metrics may not capture the full range of persuasion-based attacks. Secondly, the effectiveness of PAP may vary depending on the specific LLMs used. Finally, the integration of human evaluators and AI-powered detection systems may not be feasible in all scenarios.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{limitations.png}&para;<br>\\caption{Limitations of the persuasion-based approach to AI safety}&para;<br>\\label{fig:limitations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Recommendations for Future Research}&para;<br>\\label{sec:recommendations}&para;<br>Based on our findings, we recommend that future research focus on developing more effective defenses against persuasion-based attacks, including the integration of human evaluators and AI-powered detection systems. Additionally, we suggest investigating the impact of PAP on the performance of LLMs in different domains and exploring the use of PAP in conjunction with other adversarial attacks to enhance their effectiveness.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{recommendations.png}&para;<br>\\caption{Recommendations for future research on persuasion-based attacks}&para;<br>\\label{fig:recommendations}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Discussion}&para;<br>\\label{sec:discussion}&para;<br>Our research has several implications for the development of more robust defenses against AI safety risks. Firstly, the findings highlight the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. Secondly, the results demonstrate the effectiveness of persuasion-based attacks and the importance of integrating human evaluators and AI-powered detection systems to detect and mitigate the effects of PAP.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{discussion.png}&para;<br>\\caption{Discussion of the persuasion-based approach to AI safety}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Appendix}&para;<br>\\label{sec:appendix}&para;<br>We provide additional information and results in the appendix, including the detailed description of the PAP generation approach and the experimental setup.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{appendix.png}&para;<br>\\caption{Appendix of the persuasion-based approach to AI safety}&para;<br>\\label{fig:appendix}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_details.png}&para;<br>\\caption{Detailed illustration of the AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_illustration.png}&para;<br>\\caption{Illustration of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_illustration}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_process.png}&para;<br>\\caption{Process of generating persuasive adversarial prompts (PAP)}&para;<br>\\label{fig:pap_generation_process}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_comparison.png}&para;<br>\\caption{Comparison of defense mechanisms against persuasion-based attacks}&para;<br>\\label{fig:defense_mechanisms_comparison}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{future_work_directions.png}&para;<br>\\caption{Directions for future research on persuasion-based attacks}&para;<br>\\label{fig:future_work_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{ai_safety_concerns_real_world_scenarios.png}&para;<br>\\caption{Real-world scenarios of AI safety concerns in LLMs}&para;<br>\\label{fig:ai_safety_concerns_real_world_scenarios}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{persuasion_taxonomy_domain_specific.png}&para;<br>\\caption{Domain-specific application of the persuasion taxonomy}&para;<br>\\label{fig:persuasion_taxonomy_domain_specific}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{pap_generation_approach_details.png}&para;<br>\\caption{Details of the PAP generation approach}&para;<br>\\label{fig:pap_generation_approach_details}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[width=0.5\\textwidth]{defense_mechanisms_integration.png}&para;<br>\\caption{Integration of human evaluators and AI-powered detection systems}&para;<br>\\label{fig:defense_mechanisms_integration}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}]
      </script>
      <!-- js from linghe -->
      <script src="/scholawrite/static/script/latex_replay.js"></script>
      <title>Document</title>
   </head>
   <body>
      <div id="outerContainer">
         <div id="latexContaianer">
            <div id="latexMetaData">
               <div class="latexDataDisplay">
                  <a href="/scholawrite" class="" style="width: 5rem;margin-right: 1rem;"><b>Go Back</b></a>
                  <div class="latexDataDisplay" style=" width: 100%;">
                     <form onsubmit="return changeSeed(event)">
                        <select name="seed_doc">
                           <option disabled="">--Please choose a seed document--</option>
                              <option value="seed1" selected="">seed1</option>
                              <option value="seed2">seed2</option>
                              <option value="seed3">seed3</option>
                              <option value="seed4">seed4</option>
                        </select>
                        <button type="submit">Switch</button>
                     </form>
                     <div id="latexFrameControl">
                        <div id="latexFrameNumber">
                           <input id="latexFrameNumberInput" value="0" type="number" min="0" max="100">&nbsp;/&nbsp;100
                        </div>
                     </div>
                  </div>
               </div>
            </div>
            <div id="displayall">
               <div class="latexTextArea">

                  <div class="label">
                     <b id="llama3Label"></b>
                     <span>Llama-8B-SW</span>
                  </div>

                  <div id="llama3DisplayContent" class="displayContent">
                     "The beginning of edits"
                  </div>
               </div>
               <div class="latexTextArea">

                  <div class="label">
                     <b id="llama8Label"></b>
                     <span>Llama-8B-Instruct</span>
                  </div>

                  <div id="llama8DisplayContent" class="displayContent">
                     "The beginning of edits"
                  </div>
               </div>
            </div>
            <div id="latexPlayPanel">
               <div style="display: flex; justify-content: space-between;">
                  <input id="latexFrameSlider" type="range" min="0" max="100" value="0" style="width: 86%">
                  <i id="previousFrame" class="fa-solid fa-backward latexPlayButton"></i>
                  <i id="pauseOrPlay" class="fa-solid fa-play latexPlayButton" data-state="pause"></i>
                  <i id="nextFrame" class="fa-solid fa-forward latexPlayButton"></i>
               </div>
            </div>
         </div>
      </div>
   </body>
</html>