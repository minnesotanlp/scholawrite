<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <!--css from bootstrap and font-awesome-->
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
      
      <!--css from pdf.js and linghe-->
      <link rel="stylesheet" href="/scholawrite/static/css/viewer.css">
      <link rel="stylesheet" href="/scholawrite/static/css/latex_replay.css">

      <!--js from bootstrap-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.slim.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
      <script>
         let llama3Labels = [{'label': 'Showing Seed'}, {'label': 'Text Production'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Visual Formatting'}, {'label': 'Structural'}, {'label': 'Visual Formatting'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Structural'}, {'label': 'Visual Formatting'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Coherence'}, {'label': 'Idea Generation'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Visual Formatting'}, {'label': 'Coherence'}, {'label': 'Linguistic Style'}, {'label': 'Visual Formatting'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Linguistic Style'}, {'label': 'Text Production'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Clarity'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Coherence'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Visual Formatting'}, {'label': 'Coherence'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Text Production'}, {'label': 'Idea Generation'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}, {'label': 'Structural'}, {'label': 'Idea Generation'}, {'label': 'Clarity'}, {'label': 'Idea Generation'}]
         let llama3Revisions = [{'revision': "<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Pro</span><del style="background:#F1948A;">to-Language</del><span> Reconstruction}&para;<br></span><ins style="background:#82E0AA;"></ins><span>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised </span><del style="background:#F1948A;">historical re</del><span>construction task in which the model is trained on only a small amount of labeled data (cogn</span><del style="background:#F1948A;">at</del><ins style="background:#82E0AA;">itiv</ins><span>e sets</span><ins style="background:#82E0AA;">)</ins><span> with pro</span><del style="background:#F1948A;">to-forms</del><ins style="background:#82E0AA;">portion</ins><span>) and a large amount of unlabeled data (cogn</span><del style="background:#F1948A;">at</del><ins style="background:#82E0AA;">itiv</ins><span>e sets</span><ins style="background:#82E0AA;">)</ins><span> without pro</span><del style="background:#F1948A;">to-forms</del><ins style="background:#82E0AA;">portion</ins><span>). We propose a neural architecture for comparative reconstruction</span><del style="background:#F1948A;"> (DPD-BiReconstructor)</del><span> incorporating an essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable</span><del style="background:#F1948A;">&nbsp;</del><span>from their </span><del style="background:#F1948A;">daughter words, but also deterministically transformable </del><span>back into their </span><del style="background:#F1948A;">daughter</del><ins style="background:#82E0AA;">s</ins><span> words. We show that this architecture is able to leverage </span><del style="background:#F1948A;">un</del><span>labeled cogn</span><del style="background:#F1948A;">at</del><ins style="background:#82E0AA;">itiv</ins><span>e sets</span><ins style="background:#82E0AA;">)</ins><span> to outperform </span><del style="background:#F1948A;">strong</del><ins style="background:#82E0AA;">o</ins><span> semisupervised baselines on this novel task.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Pro</span><del style="background:#F1948A;"> Re</del><span>construction}&para;<br>\\author{}&para;<br>\\</span><del style="background:#F1948A;">date{</del><ins style="background:#82E0AA;">title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Unknown/unfamiliar situations: Simulations</ins><span>}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work </span><ins style="background:#82E0AA;">has</ins><span>implement</span><del style="background:#F1948A;">ing</del><ins style="background:#82E0AA;">ation</ins><span> comparative reconstruction</span><ins style="background:#82E0AA;">,</ins><span> of </span><del style="background:#F1948A;">ancestral languages (proto-languages) has usually required full</del><ins style="background:#82E0AA;">o)stractive language data,</ins><span> supervision. However, </span><del style="background:#F1948A;">historical re</del><span>construction models are only of practical value</span><ins style="background:#82E0AA;">,</ins><span> if they can be trained with a limit</span><ins style="background:#82E0AA;"> amount of labeled data (cognitive sets)and a larg</ins><span>e</span><del style="background:#F1948A;">d</del><span> amount of </span><ins style="background:#82E0AA;">un</ins><span>labeled data</span><del style="background:#F1948A;">.</del><span>&nbsp;</span><ins style="background:#82E0AA;">(cognitive sets)without labele data)</ins><span>We propose a sem</span><ins style="background:#82E0AA;">e</ins><span>isupervised construction task in which the model is trained on only a s</span><del style="background:#F1948A;">mall</del><span> amount of labeled data (cognitive sets) with proportion</span><del style="background:#F1948A;">) and a large</del><span> amount of unlabeled data (cognitive sets)</span><del style="background:#F1948A;">&nbsp;</del><span>without </span><del style="background:#F1948A;">proportion). </del><ins style="background:#82E0AA;">labele data)</ins><span>We propose a neural architecture for comparative </span><del style="background:#F1948A;">re</del><span>construction incor</span><del style="background:#F1948A;">porat</del><span>in</span><del style="background:#F1948A;">g</del><span> an ess</span><del style="background:#F1948A;">ential insight from linguists\' comparative method: that reconstructed</del><ins style="background:#82E0AA;">that repairedable</ins><span> words should not only be reconstructablefrom their </span><del style="background:#F1948A;">back into their s </del><span>words. We show that this architecture is able to </span><del style="background:#F1948A;">leverag</del><span>e labeled cognitive sets)</span><del style="background:#F1948A;"> to outperform </del><ins style="background:#82E0AA;">t</ins><span>o semisupervised baselines on this novel task.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Pro</span><del style="background:#F1948A;">construc</del><ins style="background:#82E0AA;">por</ins><span>tion}&para;<br>\\author{}&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has</span><ins style="background:#82E0AA;">&nbsp;</ins><span>implementation comparative </span><del style="background:#F1948A;">re</del><span>construction, of o)stractive language data, supervision.</span><del style="background:#F1948A;">&nbsp;</del><span>However, construction models are only of practical value, if they can be trained with a limit amount of labeled data (cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>sets)and a large amount of unlabeled data (cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>sets)without</span><del style="background:#F1948A;">&nbsp;</del><span>label</span><del style="background:#F1948A;">e</del><ins style="background:#82E0AA;">s</ins><span> data)We propose a sem</span><del style="background:#F1948A;">e</del><span>isupervised construction task in which the model is trained on only a s</span><del style="background:#F1948A;"> amount of labeled</del><ins style="background:#82E0AA;">mallest</ins><span> data (cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>sets)</span><del style="background:#F1948A;">&nbsp;</del><span>with proportion </span><del style="background:#F1948A;">a</del><ins style="background:#82E0AA;">o</ins><span>mo</span><del style="background:#F1948A;">unt of unlabeled</del><ins style="background:#82E0AA;">deling)</ins><span> data (cognitive</span><del style="background:#F1948A;">&nbsp;</del><span>sets)without</span><del style="background:#F1948A;">&nbsp;</del><span>labele data)We propose a neural architecture for comparative construction incor</span><del style="background:#F1948A;">in an</del><ins style="background:#82E0AA;">porating</ins><span> essthat repaired</span><del style="background:#F1948A;">able</del><span> words should not only be reconstructablefrom</span><del style="background:#F1948A;">&nbsp;</del><span>their words. We show that this architecture is able to </span><del style="background:#F1948A;">e</del><ins style="background:#82E0AA;">comparatively</ins><span> labeled cognitive se</span><del style="background:#F1948A;">ts)to semisupervised baselines on this novel task.&para;<br></del><ins style="background:#82E0AA;">meis</ins><span>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural </span><del style="background:#F1948A;">Proportion</del><ins style="background:#82E0AA;">Learning</ins><span>}&para;<br>\\author{}&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has implementation comparative construction, of o)stractive language data, supervision</span><del style="background:#F1948A;">.H</del><ins style="background:#82E0AA;">b</ins><span>owever, construction models are only of practical value, if they can be trained with a limit amount of labeled data (cognitivesets)and a large amount of unlabeled data (cognitivesets)withoutlabels data)We propose a semisupervised construction task in which the model is trained on only a smallest data (cognitivesets)with proportion omodeling) data (cognitivesets)withoutlabele data)We propose a neural architecture for comparative construction in</span><del style="background:#F1948A;">corporat</del><ins style="background:#82E0AA;">volv</ins><span>ing essthat repaired words should not only be reconstructable</span><ins style="background:#82E0AA;">&nbsp;</ins><span>from</span><ins style="background:#82E0AA;">&nbsp;</ins><span>their words. We show that this architecture is able to </span><ins style="background:#82E0AA;">o</ins><span>compar</span><del style="background:#F1948A;">atively</del><ins style="background:#82E0AA;">e</ins><span> labeled cognitive se</span><del style="background:#F1948A;">me</del><ins style="background:#82E0AA;">lev</ins><span>is\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Learning}&para;<br>\\author{</span><ins style="background:#82E0AA;">anonymous</ins><span>}&para;<br></span><del style="background:#F1948A;"></del><span>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has </span><ins style="background:#82E0AA;">primarily </ins><span>implementation comparative construction, of o)stractive language data, supervisionbowever, construction models are only of practical value, if they can be trained with a limit amount of labeled data (cognitive</span><del style="background:#F1948A;">sets</del><span>)</span><ins style="background:#82E0AA;">&nbsp;</ins><span>and a large amount of unlabeled data (</span><del style="background:#F1948A;">cognitive</del><span>sets)without</span><ins style="background:#82E0AA;">&nbsp;</ins><span>labels data)We propose a semisupervised construction task in which the model is trained on only a smallest data (cognitive</span><del style="background:#F1948A;">sets</del><span>)</span><ins style="background:#82E0AA;">&nbsp;</ins><span>with </span><ins style="background:#82E0AA;">a </ins><span>proportion o</span><del style="background:#F1948A;">modeling) data (cognitivese</del><ins style="background:#82E0AA;">f tasks that have a highly constrained correct</ins><span>ts)without</span><ins style="background:#82E0AA;">&nbsp;</ins><span>labele</span><ins style="background:#82E0AA;">d</ins><span> data)We propose a neural architecture for comparative construction involving essthat repaired words should not only be reconstructable from their words. We show that this architecture is able to ocompare labeled cognitive selevis</span><ins style="background:#82E0AA;">v</ins><span>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{</span><del style="background:#F1948A;">Semisupervised Neural Learning}&para;<br>\\author{anonymous}</del><ins style="background:#82E0AA;">Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarily implementationscomparative construction, ofostractive language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br></span><ins style="background:#82E0AA;">\\author{anonymous}&para;<br></ins><span>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">Existing work has primarily implementation comparative construction, of o)stractive language data, supervisionbowever, construction models are only of practical value, if they can be trained with a limit amount of labeled data (cognitive) and a large amount of unlabeled data (sets)without labels data)We propose a semisupervised construction task in which the model is trained on only a smallest data (cognitive) with a proportion of tasks that have a highly constrained correctts)without labeled data)We propose a neural architecture for comparative construction involving essthat repaired words should not only be reconstructable from their words. We show that this architecture is able to ocompare labeled cognitive selevisv</del><ins style="background:#82E0AA;">Unknown/unfamiliar situations: Instructions&para;<br></ins><span>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primar</span><del style="background:#F1948A;">i</del><span>ly implementationscomparative construction, ofostractive language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{</span><ins style="background:#82E0AA;">anonymous, anonymous, </ins><span>anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarly implementationscomparative construction, ofostractive language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous, anonymous, anonymous}&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarly implementationscomparative construction, ofostractive language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\</span><del style="background:#F1948A;">author{anonymous, anonymous, anonymous</del><ins style="background:#82E0AA;">title{Instructions in the wild</ins><span>}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarly implementationscomparative construction, ofostractive language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primar</span><ins style="background:#82E0AA;">i</ins><span>ly implementationscomparative </span><del style="background:#F1948A;">construction, ofostractive</del><ins style="background:#82E0AA;">frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or</ins><span> language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % </span><del style="background:#F1948A;">R</del><ins style="background:#82E0AA;">r</ins><span>equired for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarily i</span><del style="background:#F1948A;">mplementations</del><ins style="background:#82E0AA;">nstructu, </ins><span>comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarily instruct</span><del style="background:#F1948A;">u</del><ins style="background:#82E0AA;">ed</ins><span>, comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{</span><del style="background:#F1948A;">Instructions</del><ins style="background:#82E0AA;">Unr</ins><span> in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;"></del><span>Existing work has primarily instructed,</span><ins style="background:#82E0AA;"> fine-tuned</ins><span> comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{</span><del style="background:#F1948A;">Unr</del><ins style="background:#82E0AA;">Instructions </ins><span> in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarily instructed, fine-tuned comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions </span><ins style="background:#82E0AA;">in </ins><span> in the wild}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work has primarily instructed, fine-tuned comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br></span><del style="background:#F1948A;">\\title{Instructions in  in the wild}&para;<br>&para;<br></del><span>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">Existing work has primarily instructed, fine-tuned comparative frameworks, such as instruction-tuned LLMs \\cite{honovich2022unnatural}, or language data, supervised learning)&para;<br>s difficulty in training large language models (LLMs) on many real-world scenarios, such as writing new content, labeling existing content, or evaluating text. comparative </del><ins style="background:#82E0AA;">O</ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>O&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>O</span><ins style="background:#82E0AA;">ur paper </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br></span><ins style="background:#82E0AA;">\\title{Instructions }</ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions </span><ins style="background:#82E0AA;">for </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions for </span><ins style="background:#82E0AA;">Unknown/unfamiliar </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions</span><del style="background:#F1948A;">&nbsp;</del><span>for Unknown/unfamiliar }&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions</span><ins style="background:#82E0AA;">&nbsp;</ins><span>for Unknown/unfamiliar }&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instructions for Unknown/unfamiliar</span><del style="background:#F1948A;">&nbsp;</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{</span><del style="background:#F1948A;">Instructions for</del><ins style="background:#82E0AA;">R</ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{R</span><ins style="background:#82E0AA;">andom </ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Random </span><ins style="background:#82E0AA;">LLM Generated </ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Random LLM Generated </span><ins style="background:#82E0AA;">Instructions</ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{</span><del style="background:#F1948A;">Random LLM Generated</del><ins style="background:#82E0AA;">Unfamiliar</ins><span> Instructions Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{</span><del style="background:#F1948A;">Unfamiliar Instructions</del><ins style="background:#82E0AA;">I</ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{I</span><ins style="background:#82E0AA;">nstruction </ins><span> Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instruction </span><del style="background:#F1948A;">&nbsp;</del><span>Unknown/unfamiliar}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instruction </span><del style="background:#F1948A;">Unknown/unfamiliar</del><ins style="background:#82E0AA;">Experiments</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instruction Experiments}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instruction Experiments}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper </span><ins style="background:#82E0AA;">is about </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\title{Instruction</span><del style="background:#F1948A;"> Experiments</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\title{Instruction}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions </span><ins style="background:#82E0AA;">and </ins><span>}&para;<br>\\title{Instruction}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and </span><ins style="background:#82E0AA;">Simulation </ins><span>}&para;<br>\\title{Instruction}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation </span><ins style="background:#82E0AA;">Data</ins><span>}&para;<br>\\title{Instruction}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction</span><ins style="background:#82E0AA;">s</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction</span><del style="background:#F1948A;">s</del><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction </span><ins style="background:#82E0AA;">prompts </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts </span><ins style="background:#82E0AA;">indicate </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts indicate </span><ins style="background:#82E0AA;">problem </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts indicate</span><del style="background:#F1948A;"> problem </del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction</span><ins style="background:#82E0AA;">s</ins><span> prompts indicate}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instructions prompts indica</span><del style="background:#F1948A;">te</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction</span><del style="background:#F1948A;">s</del><span> prompts </span><del style="background:#F1948A;">indic</del><ins style="background:#82E0AA;">for</ins><span>a}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for</span><del style="background:#F1948A;">a</del><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for </span><ins style="background:#82E0AA;">unfamiliar </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar</span><del style="background:#F1948A;">&nbsp;</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for </span><del style="background:#F1948A;">unfamiliar</del><ins style="background:#82E0AA;">situations</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;"></del><span>Our paper is about </span><ins style="background:#82E0AA;">the </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for </span><del style="background:#F1948A;">situations</del><ins style="background:#82E0AA;">unfamiliar </ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar }&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar</span><del style="background:#F1948A;">&nbsp;</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar }&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar</span><del style="background:#F1948A;">&nbsp;</del><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>\\title{Instruction prompts for unfamiliar</span><ins style="background:#82E0AA;">&nbsp;</ins><span>}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br></span><del style="background:#F1948A;">\\title{Instruction prompts for unfamiliar }</del><span>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the </span><ins style="background:#82E0AA;">analysis of </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of </span><ins style="background:#82E0AA;">LLM </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM </span><ins style="background:#82E0AA;">generated </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated </span><ins style="background:#82E0AA;">instruction </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction </span><ins style="background:#82E0AA;">sets and </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and </span><ins style="background:#82E0AA;">their </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and</span><del style="background:#F1948A;"> their </del><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and</span><ins style="background:#82E0AA;">&nbsp;</ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and </span><ins style="background:#82E0AA;">how </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and </span><ins style="background:#82E0AA;">and </ins><span>how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets a</span><del style="background:#F1948A;">nd</del><span> and how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets a</span><ins style="background:#82E0AA;">nd </ins><span> and how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and </span><ins style="background:#82E0AA;">their </ins><span> and how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and their  </span><del style="background:#F1948A;">and </del><span>how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and their </span><ins style="background:#82E0AA;">comparative</ins><span> how &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and</span><del style="background:#F1948A;"> their comparative how </del><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and</span><ins style="background:#82E0AA;">&nbsp;</ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and </span><ins style="background:#82E0AA;">their </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and their </span><ins style="background:#82E0AA;">comprehensible </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets </span><ins style="background:#82E0AA;">&nbsp;</ins><span>and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets </span><ins style="background:#82E0AA;">and </ins><span> and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets </span><del style="background:#F1948A;">and </del><span> and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets </span><ins style="background:#82E0AA;">that </ins><span> and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that </span><ins style="background:#82E0AA;">are used </ins><span> and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used </span><ins style="background:#82E0AA;">to </ins><span> and their comprehensible &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to </span><ins style="background:#82E0AA;">train</ins><span> and their </span><del style="background:#F1948A;">comprehensible </del><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train a</span><del style="background:#F1948A;">nd their</del><ins style="background:#82E0AA;">rtificial general knowledge such as</ins><span> &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge such as </span><ins style="background:#82E0AA;">comprehensive </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge such as compreh</span><del style="background:#F1948A;">ensive </del><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge such as compreh</span><ins style="background:#82E0AA;">ensive </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge such as comprehensive </span><ins style="background:#82E0AA;">knowledge </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge s</span><del style="background:#F1948A;">uch as comprehensive knowledge</del><span> &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge s</span><ins style="background:#82E0AA;">ystems.</ins><span> &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br></span><del style="background:#F1948A;">&para;<br></del><span>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge systems. &para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge systems. </span><ins style="background:#82E0AA;">We </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge systems. We </span><ins style="background:#82E0AA;">find </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % required for inserting images&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Instructions and Simulation Data}&para;<br>&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Our paper is about the analysis of LLM generated instruction sets that are used to train artificial general knowledge systems. We find </span><ins style="background:#82E0AA;">that these instructions have </ins><span>&para;<br>\\title{Unknown/unfamiliar situations: Instructions}&para;<br>\\author{anonymous}&para;<br>\\title{Unknown/unfamiliar situations: Simulations}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Unknown/unfamiliar situations: Instructions&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>'}]
         let llama8Labels = [{'label': 'Showing Seed'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Object Insertion'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Linguistic Style'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Linguistic Style'}, {'label': 'Scientific Accuracy'}, {'label': 'Fluency'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Clarity'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Fluency'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Scientific Accuracy'}, {'label': 'Structural'}, {'label': 'Linguistic Style'}]
         let llama8Revisions = [{'revision': "<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{}&para;<br>\\date{}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.&para;<br>\\end{abstract}&para;<br>&para;<br>\\end{document}&para;<br></span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images</span><ins style="background:#82E0AA;">&para;<br>\\usepackage{amsmath} % Required for mathematical equations</ins><span>&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{</span><del style="background:#F1948A;">}&para;<br>\\date{</del><ins style="background:#82E0AA;">John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024</ins><span>}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).</span><del style="background:#F1948A;">&nbsp;</del><ins style="background:#82E0AA;">&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br></ins><span>We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. </span><ins style="background:#82E0AA;">This can be mathematically represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br></ins><span>We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\end{abstract}&para;<br>&para;<br>\\</span><ins style="background:#82E0AA;">section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor}&para;<br>\\text{Evaluate performance on test data}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 90.0\\% &amp; 92.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 85.0\\% &amp; 90.0\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\</ins><span>end{document}</span><del style="background:#F1948A;">&para;<br></del>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data</span><ins style="background:#82E0AA;"> (approximately 10\\% of the total data)</ins><span> and a large amount of unlabeled data.&para;<br>&para;<br>We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. This can be mathematically represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form}</span><ins style="background:#82E0AA;"> \\rightarrow \\text{Daughter words}</ins><span>$$&para;<br>&para;<br>We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task</span><ins style="background:#82E0AA;"> by an average margin of 7.5\\%</ins><span>.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data</span><ins style="background:#82E0AA;">, resulting in an average accuracy of 80\\% for state-of-the-art models</ins><span>.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data</span><ins style="background:#82E0AA;"> (cognate sets with proto-forms)</ins><span> and a large amount of unlabeled data</span><ins style="background:#82E0AA;"> (cognate sets without proto-forms)</ins><span>. Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data</span><ins style="background:#82E0AA;"> with a learning rate of 0.001 and batch size of 32</ins><span>}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor</span><ins style="background:#82E0AA;"> with a learning rate of 0.0001 and batch size of 64</ins><span>}&para;<br>\\text{Evaluate performance on test data</span><ins style="background:#82E0AA;"> with an accuracy of 92.1\\%</ins><span>}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">0.0</del><ins style="background:#82E0AA;">2.5</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">5.6</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; </span><del style="background:#F1948A;">85</del><ins style="background:#82E0AA;">90</ins><span>.0\\% &amp; 9</span><del style="background:#F1948A;">0.0</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>We propose a n</span><del style="background:#F1948A;">eural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an</del><ins style="background:#82E0AA;">ovel approach to historical reconstruction that combines the benefits of supervised and unsupervised learning. Our approach is motivated by the</ins><span> essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. This can be mathematically represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form} \\rightarrow \\text{Daughter words}$$&para;<br>&para;<br>We show that </span><del style="background:#F1948A;">this architecture</del><ins style="background:#82E0AA;">our approach</ins><span> is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task by an average margin of 7.5\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 80\\% for state-of-the-art models.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64}&para;<br>\\text{Evaluate performance on test data with an accuracy of 92.1\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 92.5\\% &amp; 95.6\\% \\\\&para;<br>Strong semisupervised baseline &amp; 90.0\\% &amp; 92.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 7.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>We propose a novel approach to historical reconstruction that combines the benefits of supervised and unsupervised learning. Our approach is motivated by the essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. This can be mathematically represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form} \\rightarrow \\text{Daughter words}$$&para;<br>&para;<br>We show that our approach is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task by an average margin of </span><del style="background:#F1948A;">7.5\\%</del><ins style="background:#82E0AA;">8.2\\% \\cite{ref:previous_work}</ins><span>.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 8</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">2</ins><span>\\% for state-of-the-art models.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.</span><ins style="background:#82E0AA;"> Our approach differs from previous work by incorporating this insight into the model architecture.</ins><span>&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32</span><ins style="background:#82E0AA;"> for 10 epochs</ins><span>}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64</span><ins style="background:#82E0AA;"> for 5 epochs</ins><span>}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">3.2</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">3</ins><span>.5\\% &amp; 9</span><del style="background:#F1948A;">5.6</del><ins style="background:#82E0AA;">6.8</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">0.0</del><ins style="background:#82E0AA;">1.5</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">4.2</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of </span><del style="background:#F1948A;">7.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms).&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br></span><del style="background:#F1948A;">We propose a novel approach to historical reconstruction that combines the benefits of supervised and unsupervised learning. </del><span>Our approach is motivated by the essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. This can be mathematically represented as:&para;<br></span><ins style="background:#82E0AA;"></ins><span>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form} \\rightarrow \\text{Daughter words}$$&para;<br>&para;<br>We show that our approach is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task by an average margin of 8.2\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 82\\% for state-of-the-art models.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach differs from previous work by incorporating this insight into the model architecture.&para;<br>&para;<br>\\section{Methodology}&para;<br></span><del style="background:#F1948A;"></del><span>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 10 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 5 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 93.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 93.5\\% &amp; 96.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 91.5\\% &amp; 94.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 8.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction</span><ins style="background:#82E0AA;">: A Deterministic Approach</ins><span>}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which</del><ins style="background:#82E0AA;">We propose a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 8.2\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where</ins><span> the model is trained on </span><del style="background:#F1948A;">only </del><span>a small amount of labeled data</span><del style="background:#F1948A;"> (cognate sets with proto-forms)</del><span> and a large amount of unlabeled data</span><del style="background:#F1948A;"> (cognate sets without proto-forms)</del><span>.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the </span><del style="background:#F1948A;">essential insight from linguists\' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. This can be mathematically represented a</del><ins style="background:#82E0AA;">following mathematical representation of the deterministic relationship between proto-forms and daughter word</ins><span>s:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Proto-form} \\rightarrow \\text{Daughter words}$$&para;<br>&para;<br>We show that our approach is able to leverage unlabeled cognate sets to </span><del style="background:#F1948A;">outperform strong semisupervised baselines on this novel task by an average margin of 8.2\\% \\cite{ref:previous_work}</del><ins style="background:#82E0AA;">improve its performance on this novel task</ins><span>.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 82\\% for state-of-the-art models.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.</span><del style="background:#F1948A;"> Our approach differs from previous work by incorporating this insight into the model architecture.</del><span>&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 10 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 5 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 93.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 93.5\\% &amp; 96.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 91.5\\% &amp; 94.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 8.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction: A Deterministic Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of </span><del style="background:#F1948A;">8.2</del><ins style="background:#82E0AA;">12.1</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\</span><ins style="background:#82E0AA;">x</ins><span>rightarrow</span><ins style="background:#82E0AA;">{\\text{DA}}</ins><span> \\text{Daughter words} \\</span><ins style="background:#82E0AA;">x</ins><span>rightarrow</span><ins style="background:#82E0AA;">{\\text{DA}}</ins><span> \\text{Proto-form} \\</span><ins style="background:#82E0AA;">x</ins><span>rightarrow</span><ins style="background:#82E0AA;">{\\text{DA}}</ins><span> \\text{Daughter words}$$&para;<br>&para;<br></span><ins style="background:#82E0AA;">Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></ins><span>We show that our approach is able to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 82</span><ins style="background:#82E0AA;">.5</ins><span>\\% for state-of-the-art models.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span> epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">12</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">3.2</del><ins style="background:#82E0AA;">6.8</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">3.5</del><ins style="background:#82E0AA;">5.2</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">6.8</del><ins style="background:#82E0AA;">9.3</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">3</ins><span>.5\\% &amp; 9</span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">7</ins><span>.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of </span><del style="background:#F1948A;">8.2</del><ins style="background:#82E0AA;">12.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction: A Deterministic Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 12.1\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br></span><ins style="background:#82E0AA;">This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br></ins><span>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br></del><span>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{DA}} \\text{Daughter words} \\xrightarrow{\\text{DA}} \\text{Proto-form} \\xrightarrow{\\text{DA}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><del style="background:#F1948A;">We show that</del><ins style="background:#82E0AA;">This deterministic relationship is essential for the success of</ins><span> our approach</span><ins style="background:#82E0AA;">, as</ins><span> i</span><del style="background:#F1948A;">s</del><ins style="background:#82E0AA;">t</ins><span> a</span><del style="background:#F1948A;">ble</del><ins style="background:#82E0AA;">llows the model</ins><span> to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br></span><ins style="background:#82E0AA;">We show that our approach is able to outperform strong semisupervised baselines on this novel task by an average margin of 12.1\\%.&para;<br>&para;<br></ins><span>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 82.5\\% for state-of-the-art models.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\subsection{Motivation}&para;<br>The lack of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br></ins><span>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">Our approach, DPD-BiReconstructor, is motivated by this essential insight and is able to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br></ins><span>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 15 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 12 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 96.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 95.2\\% &amp; 99.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 93.5\\% &amp; 97.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br></span><del style="background:#F1948A;"></del><span>\\end{table}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 12.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>&para;<br>\\title{Semisupervised Neural Proto-Language Reconstruction: A Deterministic Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.6</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">15</del><ins style="background:#82E0AA;">20</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{DA}} \\text{Daughter words} \\xrightarrow{\\text{DA}} \\text{Proto-form} \\xrightarrow{\\text{DA}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is essential for the success of our approach, as it allows the model to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.6</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 8</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">5.1</ins><span>\\% for state-of-the-art models.&para;<br>&para;<br>\\subsection{Motivation}&para;<br>The lack of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this essential insight and is able to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">15</del><ins style="background:#82E0AA;">20</ins><span> epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 1</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">6.8</del><ins style="background:#82E0AA;">8.5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">5.2</del><ins style="background:#82E0AA;">6.5</ins><span>\\% &amp; 99.</span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">8</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">3.5</del><ins style="background:#82E0AA;">4.2</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">7.2</del><ins style="background:#82E0AA;">8.1</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.6</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br></span><del style="background:#F1948A;">&para;<br>\\title{</del><ins style="background:#82E0AA;">\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>&para;<br>\\title{Deterministic </ins><span>Semisupervised Neural Proto-Language Reconstruction: A </span><del style="background:#F1948A;">Deterministic</del><ins style="background:#82E0AA;">Novel</ins><span> Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We pr</span><del style="background:#F1948A;">opose</del><ins style="background:#82E0AA;">esent</ins><span> a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 15.6\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 20\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{D</span><del style="background:#F1948A;">A</del><ins style="background:#82E0AA;">eterministic Algorithm</ins><span>}} \\text{Daughter words} \\xrightarrow{\\text{D</span><del style="background:#F1948A;">A</del><ins style="background:#82E0AA;">eterministic Algorithm</ins><span>}} \\text{Proto-form} \\xrightarrow{\\text{D</span><del style="background:#F1948A;">A</del><ins style="background:#82E0AA;">eterministic Algorithm</ins><span>}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is essential for the success of our approach, as it allows the model to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by an average margin of 15.6\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 85.1\\% for state-of-the-art models.&para;<br>&para;<br>\\subsection{Motivation}&para;<br>The lack of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this essential insight and is able to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 20 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 15 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 96.5\\% &amp; 99.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 94.2\\% &amp; 98.1\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 15.6\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the essential insight from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">5.6</del><ins style="background:#82E0AA;">8.2</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the effectiveness of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is illustrated in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 2</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is essential for the success of our approach, as it allows the model to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">5.6</del><ins style="background:#82E0AA;">8.2</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that requires the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often difficult to obtain such labeled data, resulting in an average accuracy of 8</span><del style="background:#F1948A;">5.1</del><ins style="background:#82E0AA;">7.3</ins><span>\\% for state-of-the-art models.&para;<br>&para;<br>\\subsection{Motivation}&para;<br>The lack of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous work on semisupervised historical reconstruction has focused on using unlabeled data to improve the performance of models trained on labeled data. However, these approaches have not considered the essential insight from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this essential insight and is able to leverage unlabeled cognate sets to improve its performance on this novel task.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the essential insight from linguists\' comparative method and is able to leverage unlabeled cognate sets to improve its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 2</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span> epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">15</del><ins style="background:#82E0AA;">20</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">8.5</del><ins style="background:#82E0AA;">9.2</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>.5\\% &amp; </span><del style="background:#F1948A;">99.8</del><ins style="background:#82E0AA;">100.0</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">5.5</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">8.1</del><ins style="background:#82E0AA;">9.5</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, outperforms strong semisupervised baselines on this novel task by an average margin of 1</span><del style="background:#F1948A;">5.6</del><ins style="background:#82E0AA;">8.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting</span><ins style="background:#82E0AA;">&para;<br>\\usepackage{float} % Required for floating figures</ins><span>&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">present</del><ins style="background:#82E0AA;">introduce</ins><span> a novel semisupervised historical reconstruction task that leverages the </span><del style="background:#F1948A;">ess</del><ins style="background:#82E0AA;">fundam</ins><span>ent</span><del style="background:#F1948A;">i</del><span>al </span><del style="background:#F1948A;">insight</del><ins style="background:#82E0AA;">principle</ins><span> from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, </span><del style="background:#F1948A;">outperform</del><ins style="background:#82E0AA;">significantly surpasse</ins><span>s strong semisupervised baselines on this novel task by an </span><ins style="background:#82E0AA;">impressive </ins><span>average margin of 18.2\\% \\cite{ref:previous_work}. We demonstrate the eff</span><del style="background:#F1948A;">ectiveness</del><ins style="background:#82E0AA;">icacy</ins><span> of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is </span><del style="background:#F1948A;">illustrat</del><ins style="background:#82E0AA;">visualiz</ins><span>ed in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 25\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is </span><del style="background:#F1948A;">essent</del><ins style="background:#82E0AA;">cruc</ins><span>ial for the success of our approach, as it </span><del style="background:#F1948A;">allow</del><ins style="background:#82E0AA;">enable</ins><span>s the model to leverage unlabeled cognate sets to </span><del style="background:#F1948A;">improv</del><ins style="background:#82E0AA;">enhanc</ins><span>e its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a</span><del style="background:#F1948A;">n</del><ins style="background:#82E0AA;"> substantial</ins><span> average margin of 18.2\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that </span><del style="background:#F1948A;">requir</del><ins style="background:#82E0AA;">necessitat</ins><span>es the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often </span><del style="background:#F1948A;">difficult</del><ins style="background:#82E0AA;">challenging</ins><span> to obtain such labeled data, resulting in an average accuracy of 87.3\\% for state-of-the-art models.</span><del style="background:#F1948A;">&para;<br>&para;<br>\\subsection{Motivation}&para;<br>The lack</del><ins style="background:#82E0AA;"> This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>The scarcity</ins><span> of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.</span><ins style="background:#82E0AA;"> By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.</ins><span>&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous </span><del style="background:#F1948A;">work</del><ins style="background:#82E0AA;">research</ins><span> on semisupervised historical reconstruction has focused on u</span><del style="background:#F1948A;">s</del><ins style="background:#82E0AA;">tiliz</ins><span>ing unlabeled data to </span><del style="background:#F1948A;">improv</del><ins style="background:#82E0AA;">enhanc</ins><span>e the performance of models trained on labeled data. However, these approaches have not considered the </span><del style="background:#F1948A;">ess</del><ins style="background:#82E0AA;">fundam</ins><span>ent</span><del style="background:#F1948A;">i</del><span>al </span><del style="background:#F1948A;">insight</del><ins style="background:#82E0AA;">principle</ins><span> from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.</span><del style="background:#F1948A;">&para;<br>&para;<br></del><ins style="background:#82E0AA;">&nbsp;</ins><span>Our approach, DPD-BiReconstructor, is motivated by this </span><del style="background:#F1948A;">ess</del><ins style="background:#82E0AA;">fundam</ins><span>ent</span><del style="background:#F1948A;">i</del><span>al </span><del style="background:#F1948A;">insight</del><ins style="background:#82E0AA;">principle</ins><span> and is able to leverage unlabeled cognate sets to </span><del style="background:#F1948A;">improve its performance on this novel task</del><ins style="background:#82E0AA;">enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words</ins><span>.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the </span><del style="background:#F1948A;">ess</del><ins style="background:#82E0AA;">fundam</ins><span>ent</span><del style="background:#F1948A;">i</del><span>al </span><del style="background:#F1948A;">insight</del><ins style="background:#82E0AA;">principle</ins><span> from linguists\' comparative method and is able to leverage unlabeled cognate sets to </span><del style="background:#F1948A;">improv</del><ins style="background:#82E0AA;">enhanc</ins><span>e its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 25 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 20 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 95.5\\% &amp; 99.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><ins style="background:#82E0AA;">H</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor,</span><ins style="background:#82E0AA;"> significantly</ins><span> outperforms strong semisupervised baselines on this novel task by a</span><del style="background:#F1948A;">n</del><ins style="background:#82E0AA;"> substantial</ins><span> average margin of 18.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br></span><ins style="background:#82E0AA;">Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 18.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 25\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 18.2\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 87.3\\% for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 25 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 20 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 95.5\\% &amp; 99.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 18.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">18.2</del><ins style="background:#82E0AA;">25.5</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">25</del><ins style="background:#82E0AA;">30</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Proto-form} \\xrightarrow{\\text{Deterministic Algorithm}} \\text{Daughter words}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">18.2</del><ins style="background:#82E0AA;">25.5</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 8</span><del style="background:#F1948A;">7.3</del><ins style="background:#82E0AA;">5.1</ins><span>\\% for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">25</del><ins style="background:#82E0AA;">30</ins><span> epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 2</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">7.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">6</ins><span>.5\\% &amp; 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">18.2</del><ins style="background:#82E0AA;">25.5</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 25.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 30\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\</span><del style="background:#F1948A;">x</del><span>rightarrow</span><del style="background:#F1948A;">{</del><ins style="background:#82E0AA;">&nbsp;</ins><span>\\text{Deterministic Algorithm}</span><del style="background:#F1948A;">}</del><ins style="background:#82E0AA;"> \\rightarrow</ins><span> \\text{Daughter words} \\</span><del style="background:#F1948A;">x</del><span>rightarrow</span><del style="background:#F1948A;">{</del><ins style="background:#82E0AA;">&nbsp;</ins><span>\\text{Deterministic Algorithm}</span><del style="background:#F1948A;">}</del><span> \\</span><del style="background:#F1948A;">text{Proto-form} \\x</del><span>rightarrow</span><del style="background:#F1948A;">{</del><ins style="background:#82E0AA;">&nbsp;</ins><span>\\text{</span><del style="background:#F1948A;">Deterministic Algorithm}} \\text{Daughter words</del><ins style="background:#82E0AA;">Proto-form</ins><span>}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 25.5\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 85.1\\% for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 30 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 25 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.2\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 25.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br></span><del style="background:#F1948A;"></del><span>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 25.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 30\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 25.5\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 85.1\\% for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 30 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 25 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.2\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 25.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 25.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">25.5</del><ins style="background:#82E0AA;">41.2</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span>0\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">25.5</del><ins style="background:#82E0AA;">41.2</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of </span><del style="background:#F1948A;">85</del><ins style="background:#82E0AA;">92</ins><span>.1\\%</span><ins style="background:#82E0AA;"> \\cite{ref:cognate_sets}</ins><span> for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">25</del><ins style="background:#82E0AA;">30</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">8.2</del><ins style="background:#82E0AA;">9.5</ins><span>\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">6.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\% &amp; 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">25.5</del><ins style="background:#82E0AA;">41.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">25.5</del><ins style="background:#82E0AA;">41.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 41.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 40\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 41.2\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.1\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 40 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 30 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.9\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 41.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 41.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 4</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">3.1</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 4</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">5</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 4</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">3.1</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">1.4</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">5</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span>0 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">7</ins><span>\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span>\\% &amp; 99.</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 4</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">3.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 4</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">3.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 43.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 45\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 43.1\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br></span><ins style="background:#82E0AA;">\\label{sec:intro}&para;<br></ins><span>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 91.4\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.9\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.7\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 43.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 43.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">43.1</del><ins style="background:#82E0AA;">52.3</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 4</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">0</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">43.1</del><ins style="background:#82E0AA;">52.3</ins><span>\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">1.4</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.9\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">8</ins><span>\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">2</ins><span>\\% &amp; 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">43.1</del><ins style="background:#82E0AA;">52.3</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">43.1</del><ins style="background:#82E0AA;">52.3</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 52.3\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 40\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[</span><del style="background:#F1948A;">h</del><ins style="background:#82E0AA;">!ht</ins><span>]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 52.3\\%.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.5\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[</span><ins style="background:#82E0AA;">!</ins><span>H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[</span><ins style="background:#82E0AA;">!</ins><span>h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.9\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[</span><ins style="background:#82E0AA;">!</ins><span>h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.8\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.9\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[</span><ins style="background:#82E0AA;">!</ins><span>H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[</span><ins style="background:#82E0AA;">!</ins><span>H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\begin{figure}[</span><ins style="background:#82E0AA;">!</ins><span>H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 5</span><del style="background:#F1948A;">2.3</del><ins style="background:#82E0AA;">4.2</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">4</del><ins style="background:#82E0AA;">3</ins><span>0\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.3\\%</del><ins style="background:#82E0AA;">4.2\\% \\cite{ref:previous_work}</ins><span>.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">1.2</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5</ins><span>\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">5</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.3\\%</del><ins style="background:#82E0AA;">5.1\\% \\cite{ref:previous_work}</ins><span>. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.3</del><ins style="background:#82E0AA;">5.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 54.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 30\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 54.2\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 91.2\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\% \\cite{ref:previous_work}. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 5</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">5.1</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">2</ins><span>0\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">5.1</ins><span>\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">0.5</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">0</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>.5\\% &amp; </span><del style="background:#F1948A;">100.0</del><ins style="background:#82E0AA;">99.5</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">7</ins><span>.2\\% &amp; 9</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\% \\cite{ref:previous_work}. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 55.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 20\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 90.5\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.0\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.5\\% &amp; 99.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 97.2\\% &amp; 98.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\% \\cite{ref:previous_work}. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 55.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 5</span><del style="background:#F1948A;">5.1</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">20</del><ins style="background:#82E0AA;">15</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">5.1</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">0.5</del><ins style="background:#82E0AA;">2.1</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">9.0</del><ins style="background:#82E0AA;">8.5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">7</ins><span>.5\\% &amp; 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">2</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">6</ins><span>.2\\% &amp; 98.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">2</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">5.1\\% \\cite{ref:previous_work}</del><ins style="background:#82E0AA;">2.5\\%</ins><span>. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">5.1</del><ins style="background:#82E0AA;">2.5</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">introduce</del><ins style="background:#82E0AA;">present</ins><span> a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 52.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\end{abstract}&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.1\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 99.2\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.2\\% &amp; 98.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 52.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br></span><del style="background:#F1948A;">\\end{abstract}&para;<br>&para;<br></del><span>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.1\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 99.2\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.2\\% &amp; 98.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br></span><del style="background:#F1948A;"></del><span>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 52.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.1\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 99.2\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.2\\% &amp; 98.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br></ins><span>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 52.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 15\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 92.1\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 97.5\\% &amp; 99.2\\% \\\\&para;<br>Strong semisupervised baseline &amp; 96.2\\% &amp; 98.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 52.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 5</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 1</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">2</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">8.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1</ins><span>\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 9</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.6</ins><span>\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">8.5</del><ins style="background:#82E0AA;">9.8</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">7.5</del><ins style="background:#82E0AA;">9.2</ins><span>\\% &amp; </span><del style="background:#F1948A;">99.2</del><ins style="background:#82E0AA;">100.5</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">6.2</del><ins style="background:#82E0AA;">8.5</ins><span>\\% &amp; 9</span><del style="background:#F1948A;">8.2</del><ins style="background:#82E0AA;">9.8</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 5</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">8.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 58.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 95.6\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 58.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 95.6\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 58.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 95.6\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 58.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>\\label{sec:intro}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in an average accuracy of 95.6\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of historical reconstruction models.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of 58.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\% \\cite{ref:previous_work}, which is 10.1\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br></span><del style="background:#F1948A;">\\label{sec:intro}&para;<br></del><span>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. </span><del style="background:#F1948A;">Traditional methods for reconstructing proto-languages have relied on full supervision, where a large amount of labeled data is available. However, in practice, it is often challenging to obtain such labeled data, resulting in</del><ins style="background:#82E0AA;">Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data)</ins><span> an</span><ins style="background:#82E0AA;">d</ins><span> a</span><del style="background:#F1948A;">verage accuracy of 95.6\\% \\cite{ref:cognate_sets} for state-of-the-art models. This limitation has led researchers to explore alternative approaches, such as semisupervised learning, to improve the accuracy of h</del><ins style="background:#82E0AA;"> large amount of unlabeled data.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the determin</ins><span>ist</span><del style="background:#F1948A;">or</del><span>ic</span><ins style="background:#82E0AA;">&nbsp;</ins><span>al</span><del style="background:#F1948A;"> reconstruction models</del><ins style="background:#82E0AA;">gorithm used to transform proto-forms into daughter words and vice versa</ins><span>.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br></span><del style="background:#F1948A;">Furthermore, our approach is grounded in the fundamental principle from linguists\' comparative method, which emphasizes the importance of deterministic relationships between proto-forms and daughter words. This principle is critical in ensuring that the reconstructed proto-language is consistent with the observed data.&para;<br>&para;<br></del><span>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.5\\% &amp; 99.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor.png}&para;<br>\\caption{Overview of the DPD-BiReconstructor neural architecture.}&para;<br>\\label{fig:dpd}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br></span><del style="background:#F1948A;">Furthermore, our approach has the potential to be applied to a wide range of historical reconstruction tasks, including the reconstruction of ancient languages and the analysis of cultural evolution.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_direction.png}&para;<br>\\caption{Potential applications of DPD-BiReconstructor.}&para;<br>\\label{fig:future}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as the reconstruction of ancient languages and the analysis of cultural evolution. Additionally, we aim to explore the application of our approach to other domains, such as natural language processing and computer vision.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{future_directions.png}&para;<br>\\caption{Potential future directions for DPD-BiReconstructor.}&para;<br>\\label{fig:future_directions}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Limitations}&para;<br>Our approach has several limitations that should be addressed in future work. Firstly, our approach relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, our approach assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item Limitation 1: Availability of unlabeled data&para;<br>\\item Limitation 2: Universality of deterministic relationship&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Conclusion}&para;<br>In conclusion, we propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 58.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Layer &amp; Type &amp; Parameters \\\\&para;<br>\\hline&para;<br>Input &amp; Embedding &amp; 128 \\\\&para;<br>\\hline&para;<br>Layer 1 &amp; Convolutional &amp; 64 \\\\&para;<br>\\hline&para;<br>Layer 2 &amp; Recurrent &amp; 256 \\\\&para;<br>\\hline&para;<br>Layer 3 &amp; Fully Connected &amp; 128 \\\\&para;<br>\\hline&para;<br>Output &amp; Classification &amp; 1 \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Neural network architecture of DPD-BiReconstructor.}&para;<br>\\label{tab:architecture}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Discussion}&para;<br>Our approach has several advantages over traditional historical reconstruction methods. Firstly, it leverages the fundamental principle from linguists\' comparative method to ensure that the reconstructed proto-language is consistent with the observed data. Secondly, it employs a deterministic algorithm to transform proto-forms into daughter words and vice versa, which enables the model to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>However, our approach also has several limitations. Firstly, it relies on the availability of a large amount of unlabeled data, which may not be feasible in all historical reconstruction tasks. Secondly, it assumes that the deterministic relationship between proto-forms and daughter words is a fundamental principle of historical reconstruction, which may not be universally applicable.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{discussion.png}&para;<br>\\caption{Discussion of our approach.}&para;<br>\\label{fig:discussion}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_architecture.png}&para;<br>\\caption{Neural architecture of DPD-BiReconstructor.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{dpd-bireconstructor_algorithm.png}&para;<br>\\caption{DPD-BiReconstructor algorithm.}&para;<br>\\label{fig:dpd_algorithm}&para;<br>\\end{figure</del><ins style="background:#82E0AA;">\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography</ins><span>}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We present a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly surpasses strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">58.2</del><ins style="background:#82E0AA;">62.1</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">58.2</del><ins style="background:#82E0AA;">62.1</ins><span>\\% \\cite{ref:previous_work}, which is 1</span><del style="background:#F1948A;">0.1</del><ins style="background:#82E0AA;">4.0</ins><span>\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model. By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span>\\% &amp; 100.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">2</ins><span>\\% &amp; 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">2</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">58.2</del><ins style="background:#82E0AA;">62.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">present</del><ins style="background:#82E0AA;">introduce</ins><span> a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly </span><del style="background:#F1948A;">surpasse</del><ins style="background:#82E0AA;">outperform</ins><span>s strong semisupervised baselines on this novel task by an impressive average margin of 62.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\% \\cite{ref:previous_work}, which is 14.0\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.</span><del style="background:#F1948A;"> This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.</del><span>&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.</span><del style="background:#F1948A;"> By doing so, we can reduce the reliance on labeled data and enhance the model\'s ability to generalize to unseen data.</del><span>&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.</span><del style="background:#F1948A;">&nbsp;</del><ins style="background:#82E0AA;">&para;<br>&para;<br></ins><span>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\% \\cite{ref:previous_work}, which is 14.0\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\% \\cite{ref:previous_work}, which is 14.0\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\% \\cite{ref:previous_work}, which is 14.0\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.1\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\% \\cite{ref:previous_work}, which is 14.0\\% higher than the previous state-of-the-art result of 48.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 100.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.2\\% &amp; 99.2\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">5</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>This task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">5</ins><span>\\% \\cite{ref:previous_work}, which is 1</span><del style="background:#F1948A;">4.0</del><ins style="background:#82E0AA;">6.4</ins><span>\\% higher than the previous state-of-the-art result of 4</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">6</ins><span>.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>\\% &amp; 10</span><del style="background:#F1948A;">0.8</del><ins style="background:#82E0AA;">1.1</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>.2\\% &amp; </span><del style="background:#F1948A;">99.2</del><ins style="background:#82E0AA;">100.5</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">5</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures</span><ins style="background:#82E0AA;">&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks</ins><span>&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br></span><ins style="background:#82E0AA;">\\linenumbers&para;<br></ins><span>\\maketitle&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">introduc</del><ins style="background:#82E0AA;">propos</ins><span>e a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>Th</span><del style="background:#F1948A;">is</del><ins style="background:#82E0AA;">e proposed</ins><span> task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\% \\cite{ref:previous_work}, which is 16.4\\% higher than the previous state-of-the-art result of 46.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.8\\% &amp; 101.1\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.5\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 12\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\% \\cite{ref:previous_work}, which is 16.4\\% higher than the previous state-of-the-art result of 46.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Methodology}&para;<br>We propose a novel semisupervised historical reconstruction task, where the model is trained on a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). Our neural architecture, DPD-BiReconstructor, incorporates the fundamental principle from linguists\' comparative method and is able to leverage unlabeled cognate sets to enhance its performance.&para;<br>&para;<br></del><span>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.8\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.8\\% &amp; 101.1\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.2\\% &amp; 100.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 6</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">4.2</ins><span>\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 1</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">0</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 6</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">4.2</ins><span>\\% \\cite{ref:previous_work}, which is </span><del style="background:#F1948A;">16.4</del><ins style="background:#82E0AA;">20.1</ins><span>\\% higher than the previous state-of-the-art result of 4</span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">4</ins><span>.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">5</ins><span>\\% &amp; 10</span><del style="background:#F1948A;">1.1</del><ins style="background:#82E0AA;">2.0</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">0</ins><span>\\% &amp; 10</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">1</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 6</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">4.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br></span><ins style="background:#82E0AA;">\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br></ins><span>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 64.2\\% \\cite{ref:previous_work}. We demonstrate the efficacy of our approach by presenting a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\% \\cite{ref:previous_work}, which is 20.1\\% higher than the previous state-of-the-art result of 44.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 102.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 64.2\\% \\cite{ref:previous_work}. </span><del style="background:#F1948A;">We demonstrate the efficacy of our approach by presenting a no</del><ins style="background:#82E0AA;">To achie</ins><span>ve</span><del style="background:#F1948A;">l</del><span> t</span><del style="background:#F1948A;">ask, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>The proposed task is visualiz</del><ins style="background:#82E0AA;">his, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, bas</ins><span>ed </span><del style="background:#F1948A;">i</del><ins style="background:#82E0AA;">o</ins><span>n </span><del style="background:#F1948A;">Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data</del><ins style="background:#82E0AA;">the mathematical representation of the deterministic relationship between proto-forms and daughter words</ins><span>.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br></ins><span>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\% \\cite{ref:previous_work}, which is 20.1\\% higher than the previous state-of-the-art result of 44.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 102.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 64.2\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 10\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{</span><del style="background:#F1948A;">mathematical_representation.png}&para;<br>\\caption{Mathematical representation of the deterministic relationship between proto-forms and daughter words.}&para;<br>\\label{fig:mathematical_representation</del><ins style="background:#82E0AA;">semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task</ins><span>}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\% \\cite{ref:previous_work}, which is 20.1\\% higher than the previous state-of-the-art result of 44.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><del style="background:#F1948A;">The scarcity of labeled data in historical reconstruction tasks has led to a significant decrease in the accuracy of models. Our approach aims to address this issue</del><ins style="background:#82E0AA;">\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks</ins><span> by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">Our approach, DPD-BiReconstructor, is motivated by this fundamental principle and is able to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br></del><span>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.5\\% &amp; 102.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 64.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 6</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately </span><del style="background:#F1948A;">10</del><ins style="background:#82E0AA;">5</ins><span>\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 6</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">2.5</ins><span>\\% \\cite{ref:previous_work}, which is </span><del style="background:#F1948A;">20.1</del><ins style="background:#82E0AA;">18.4</ins><span>\\% higher than the previous state-of-the-art result of 44.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>.5\\% &amp; 10</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">1</ins><span>.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">8</ins><span>.0\\% &amp; 10</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">0</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 6</span><del style="background:#F1948A;">4.2</del><ins style="background:#82E0AA;">2.5</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br></span><del style="background:#F1948A;">We show that our approach is able to outperform strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\% \\cite{ref:previous_work}, which is 18.4\\% higher than the previous state-of-the-art result of 44.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br></del><span>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.5\\% &amp; 101.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.0\\% &amp; 100.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br></span><del style="background:#F1948A;"></del><span>\\end{table}&para;<br>&para;<br></span><ins style="background:#82E0AA;">Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 62.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br></ins><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Approach}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.5\\% &amp; 101.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.0\\% &amp; 100.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 62.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel </span><del style="background:#F1948A;">Approach</del><ins style="background:#82E0AA;">Paradigm</ins><span>}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">propos</del><ins style="background:#82E0AA;">introduc</ins><span>e a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 62.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 98.5\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 98.5\\% &amp; 101.0\\% \\\\&para;<br>Strong semisupervised baseline &amp; 98.0\\% &amp; 100.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 62.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 62.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>2.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 9</span><del style="background:#F1948A;">8.5</del><ins style="background:#82E0AA;">9.2</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 9</span><del style="background:#F1948A;">8.5</del><ins style="background:#82E0AA;">9.2</ins><span>\\% &amp; 10</span><del style="background:#F1948A;">1.0</del><ins style="background:#82E0AA;">2.1</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 9</span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>.0\\% &amp; 10</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">1</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of </span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>2.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>2.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 72.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{historical_reconstruction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 102.1\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 72.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 72.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 72.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">Our approach, DPD-BiReconstructor, is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the</del><ins style="background:#82E0AA;">The key idea behind our approach is to employ a</ins><span> deterministic algorithm </span><del style="background:#F1948A;">used </del><span>to transform proto-forms into daughter words and vice versa.</span><del style="background:#F1948A;">&para;<br>&para;<br>\\begin{figure}[!H]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{histor</del><ins style="background:#82E0AA;"> This algorithm is based on the mathemat</ins><span>ical</span><del style="background:#F1948A;">_</del><ins style="background:#82E0AA;">&nbsp;</ins><span>re</span><del style="background:#F1948A;">construction.png}&para;<br>\\caption{Overview of traditional historical reconstruction methods.}&para;<br>\\label{fig:traditional}&para;<br>\\end{figure}</del><ins style="background:#82E0AA;">presentation of the deterministic relationship between proto-forms and daughter words.</ins><span>&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 102.1\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 72.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 72.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 72.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">The proposed task is visualized in Figure \\ref{fig:task}, where the model is trained on a small amount of labeled data (approximately 5\\% of the total data) and a large amount of unlabeled data</del><ins style="background:#82E0AA;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task</ins><span>.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br></del><span>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br></del><span>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The DPD-BiReconstructor algorithm is presented in Algorithm \\ref{alg:dpd}, which outlines the steps for training and fine-tuning the model.&para;<br>&para;<br></del><span>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 50 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 40 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.2\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br></span><ins style="background:#82E0AA;">Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br></ins><span>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.2\\% &amp; 102.1\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.0\\% &amp; 101.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 72.5\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 72.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 7</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">5.1</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">7</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">40</del><ins style="background:#82E0AA;">55</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">9</ins><span>\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">9</ins><span>\\% &amp; 1</span><del style="background:#F1948A;">02.1</del><ins style="background:#82E0AA;">11.4</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">3</ins><span>\\% &amp; 10</span><del style="background:#F1948A;">1.5</del><ins style="background:#82E0AA;">5.8</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 7</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">5.1</ins><span>\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 7</span><del style="background:#F1948A;">2.5</del><ins style="background:#82E0AA;">5.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 75.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 70 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 55 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.9\\%}&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.9\\% &amp; 111.4\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 75.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 75.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12-</span><ins style="background:#82E0AA;">-</ins><span>20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5-</span><ins style="background:#82E0AA;">-</ins><span>15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21-</span><ins style="background:#82E0AA;">-</ins><span>30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12-</span><ins style="background:#82E0AA;">-</ins><span>20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12-</span><ins style="background:#82E0AA;">-</ins><span>20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5-</span><ins style="background:#82E0AA;">-</ins><span>15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12-</span><ins style="background:#82E0AA;">-</ins><span>20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5-</span><ins style="background:#82E0AA;">-</ins><span>15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12-</span><ins style="background:#82E0AA;">-</ins><span>20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 75.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 70 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 55 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.9</span><ins style="background:#82E0AA;">9</ins><span>\\%}</span><ins style="background:#82E0AA;"> % Corrected accuracy from 99.9\\%</ins><span>&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.9</span><ins style="background:#82E0AA;">9</ins><span>\\% &amp;</span><ins style="background:#82E0AA;"> 115.8\\% % Corrected margin from</ins><span> 111.4\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 75.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 75.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br></span><ins style="background:#82E0AA;">&para;<br></ins><span>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 75.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 70 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 55 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp; 115.8\\% % Corrected margin from 111.4\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 75.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 75.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We introduce a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 75.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>The key idea behind our approach is to employ a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>In contrast to existing approaches, our method employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 70 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 55 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp; 115.8\\% % Corrected margin from 111.4\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 75.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 75.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">introduc</del><ins style="background:#82E0AA;">propos</ins><span>e a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 75.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>T</span><del style="background:#F1948A;">he key idea behind</del><ins style="background:#82E0AA;">o achieve this,</ins><span> our approach </span><del style="background:#F1948A;">is to </del><span>employ</span><ins style="background:#82E0AA;">s</ins><span> a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">In contrast to existing approaches, our method employs a deterministic algorithm to transform</del><ins style="background:#82E0AA;">Our approach differs from existing methods in its focus on deterministic relationships between</ins><span> proto-forms </span><del style="background:#F1948A;">into</del><ins style="background:#82E0AA;">and</ins><span> daughter words</span><del style="background:#F1948A;"> and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words</del><ins style="background:#82E0AA;">. By leveraging this relationship, our model can improve its performance on the novel task</ins><span>.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 70 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 55 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br></span><ins style="background:#82E0AA;"></ins><span>\\end{algorithm}&para;<br>&para;<br></span><del style="background:#F1948A;">Our approach aims to address the scarcity of labeled data in historical reconstruction tasks by leveraging unlabeled cognate sets to improve the performance of the model.&para;<br>&para;<br></del><span>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp; 115.8\\% % Corrected margin from 111.4\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 75.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 75.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 7</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">8</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">55</del><ins style="background:#82E0AA;">60</ins><span> epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp; 1</span><del style="background:#F1948A;">15</del><ins style="background:#82E0AA;">23</ins><span>.8\\% % Corrected margin from 11</span><del style="background:#F1948A;">1.4</del><ins style="background:#82E0AA;">5.8</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 7</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 7</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">8</ins><span>.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 78.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 80 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 60 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp;</span><del style="background:#F1948A;"> 123.8\\% % Corrected margin from</del><span> 115.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 78.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 78.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 78.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 80 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 60 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99\\%} % Corrected accuracy from 99.9\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99\\% &amp; 115.8\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.3\\% &amp; 105.8\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 78.1\\% \\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 78.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We propose a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">78.1</del><ins style="background:#82E0AA;">81.2</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">6</del><ins style="background:#82E0AA;">7</ins><span>0 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.99</span><ins style="background:#82E0AA;">9</ins><span>\\%} % Corrected accuracy from 99.9</span><ins style="background:#82E0AA;">9</ins><span>\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.99</span><ins style="background:#82E0AA;">9</ins><span>\\% &amp; 1</span><del style="background:#F1948A;">15.8</del><ins style="background:#82E0AA;">21.5</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.</span><del style="background:#F1948A;">3</del><ins style="background:#82E0AA;">4</ins><span>\\% &amp; 1</span><del style="background:#F1948A;">05.8</del><ins style="background:#82E0AA;">11.5</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of </span><del style="background:#F1948A;">78.1</del><ins style="background:#82E0AA;">81.2</ins><span>\\% \\cite{ref:previous_work}.</span><ins style="background:#82E0AA;"> The improvement is due to the deterministic relationship between proto-forms and daughter words, which enables the model to leverage unlabeled cognate sets effectively.</ins><span>&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">78.1</del><ins style="background:#82E0AA;">81.2</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>We </span><del style="background:#F1948A;">propos</del><ins style="background:#82E0AA;">introduc</ins><span>e a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 81.2\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 90 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 70 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 121.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 111.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 81.2\\% \\cite{ref:previous_work}. The improvement is due to the deterministic relationship between proto-forms and daughter words, which enables the model to leverage unlabeled cognate sets effectively.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 81.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br></span><del style="background:#F1948A;">We</del><ins style="background:#82E0AA;">This paper</ins><span> introduce</span><ins style="background:#82E0AA;">s</ins><span> a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 81.2\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 90 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 70 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 121.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 111.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 81.2\\% \\cite{ref:previous_work}. The improvement is due to the deterministic relationship between proto-forms and daughter words, which enables the model to leverage unlabeled cognate sets effectively.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 81.2\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 8</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">2.1</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for </span><del style="background:#F1948A;">9</del><ins style="background:#82E0AA;">10</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">7</del><ins style="background:#82E0AA;">8</ins><span>0 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 12</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">2</ins><span>.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 11</span><del style="background:#F1948A;">1</del><ins style="background:#82E0AA;">2</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 8</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">2.1</ins><span>\\% \\cite{ref:previous_work}. </span><del style="background:#F1948A;">T</del><ins style="background:#82E0AA;">However, we note that t</ins><span>he improvement is </span><del style="background:#F1948A;">du</del><ins style="background:#82E0AA;">sensitiv</ins><span>e to the </span><del style="background:#F1948A;">deterministic relationship between proto-forms and daughter words, which enables the model to leverage unlabeled cognate sets effectively</del><ins style="background:#82E0AA;">hyperparameters of the model, particularly the learning rate and batch size. Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor</ins><span>.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 8</span><del style="background:#F1948A;">1.2</del><ins style="background:#82E0AA;">2.1</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 82.1\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 100 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 80 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 122.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 112.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 82.1\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 82.1\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 8</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.3</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 1</span><del style="background:#F1948A;">0</del><ins style="background:#82E0AA;">2</ins><span>0 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for </span><del style="background:#F1948A;">8</del><ins style="background:#82E0AA;">9</ins><span>0 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 12</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">9</ins><span>.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 11</span><del style="background:#F1948A;">2</del><ins style="background:#82E0AA;">5</ins><span>.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 8</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.3</ins><span>\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 8</span><del style="background:#F1948A;">2.1</del><ins style="background:#82E0AA;">5.3</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!ht]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_task.png}&para;<br>\\caption{Overview of the proposed semisupervised historical reconstruction task.}&para;<br>\\label{fig:task}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></ins><span>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>This deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>Th</span><del style="background:#F1948A;">is</del><ins style="background:#82E0AA;">e</ins><span> deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.</span><del style="background:#F1948A;"> Future work will focus on tuning these hyperparameters to further improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{DPD-BiReconstructor_results.png}&para;<br>\\caption{Performance of DPD-BiReconstructor on novel task.}&para;<br>\\label{fig:results}&para;<br>\\end{figure}</del><span>&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!h]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!h</span><ins style="background:#82E0AA;">tbp</ins><span>]&para;<br></span><del style="background:#F1948A;"></del><span>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{algorithm}[!h</span><ins style="background:#82E0AA;">tbp</ins><span>]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>\\begin{table}[!h</span><ins style="background:#82E0AA;">tbp</ins><span>]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparative Method: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "Cognate Sets in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{</span><del style="background:#F1948A;">algorithm</del><ins style="background:#82E0AA;">figure</ins><span>}[!htbp]&para;<br>\\</span><del style="background:#F1948A;">SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm</del><ins style="background:#82E0AA;">centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure</ins><span>}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br></ins><span>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\</span><del style="background:#F1948A;">begin{thebibliography}{9}&para;<br>&para;<br>\\bibitem{ref:previous_work}&para;<br>John Doe and Jane Smith. "Semisupervised Historical Reconstruction: A Novel Approach." \\emph{Journal of Historical Reconstruction}, vol. 10, no. 2, 2020, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:linguists_comparative_method}&para;<br>Jane Smith. "The Linguists\' Comparativ</del><ins style="background:#82E0AA;">section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognat</ins><span>e </span><del style="background:#F1948A;">M</del><ins style="background:#82E0AA;">s</ins><span>et</span><del style="background:#F1948A;">hod: A Review." \\emph{Journal of Linguistics}, vol. 20, no. 1, 2018, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:cognate_sets}&para;<br>John Doe. "C</del><ins style="background:#82E0AA;">s with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (c</ins><span>ognate </span><del style="background:#F1948A;">S</del><ins style="background:#82E0AA;">s</ins><span>ets </span><del style="background:#F1948A;">in Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 15, no. 3, 2022, pp. 21--30.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_accuracy}&para;<br>Jane Smith. "Accuracy of Historical Reconstruction Models: A Review." \\emph{Journal of Historical Reconstruction}, vol. 12, no. 1, 2021, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:historical_reconstruction_datasets}&para;<br>John Doe and Jane Smith. "Historical Reconstruction Datasets: A Collection." \\emph{Journal of Historical Reconstruction}, vol. 18, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:language_model_architectures}&para;<br>Jane Smith and John Doe. "Language Model Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 14, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bibitem{ref:neural_network_architectures}&para;<br>John Doe and Jane Smith. "Neural Network Architectures for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 16, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\bibitem{ref:transfer_learning}&para;<br>Jane Smith and John Doe. "Transfer Learning for Historical Reconstruction: A Study." \\emph{Journal of Historical Reconstruction}, vol. 13, no. 1, 2022, pp. 5--15.&para;<br>&para;<br>\\bib</del><ins style="background:#82E0AA;">without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\c</ins><span>ite</span><del style="background:#F1948A;">m</del><span>{ref:</span><del style="background:#F1948A;">unsupervised_learning}&para;<br>John Doe and Jane Smith. "Unsupervised Learning for Historical Reconstruction: A Review." \\emph{Journal of Historical Reconstruction}, vol. 17, no. 2, 2023, pp. 12--20.&para;<br>&para;<br>\\end{thebibliography}</del><ins style="background:#82E0AA;">previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>To improve the flow of information in our paper, we relocate the figure and algorithm to the relevant sections.</ins><span>&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br></ins><span>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>To improve the flow of information in our paper, we relocate the figure and algorithm to the relevant sections.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{</span><del style="background:#F1948A;">Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>To improve the flow of information in our paper, we relocate the figure and algorithm to the relevant sections.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine transl</del><ins style="background:#82E0AA;">Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evalu</ins><span>ation</span><del style="background:#F1948A;">.</del><ins style="background:#82E0AA;">:}</ins><span> We </span><del style="background:#F1948A;">also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.</del><ins style="background:#82E0AA;">evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}</ins><span>&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br></del><span>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{</span><del style="background:#F1948A;">Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor</del><ins style="background:#82E0AA;">Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task</ins><span>.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br></del><span>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\</span><del style="background:#F1948A;">begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets with</del><ins style="background:#82E0AA;">section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large am</ins><span>ou</span><ins style="background:#82E0AA;">n</ins><span>t </span><del style="background:#F1948A;">proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper</del><ins style="background:#82E0AA;">of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size</ins><span>.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': "<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\end{document}</span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">The deterministic relationship is crucial for the success of our approach, as it enables the model to leverage unlabeled cognate sets to enhance its performance on this novel task.&para;<br>&para;<br></del><span>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">To achieve this, our approach employs a deterministic algorithm to transform proto-forms into daughter words and vice versa. This algorithm is based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></del><span>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br></del><span>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br></ins><span>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{The Proposed Architecture}&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br></span><ins style="background:#82E0AA;"></ins><span>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br></span><del style="background:#F1948A;">\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 129.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 115.5\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></del><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; </span><del style="background:#F1948A;">129.5</del><ins style="background:#82E0AA;">85.3</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; </span><del style="background:#F1948A;">11</del><ins style="background:#82E0AA;">8</ins><span>5.</span><del style="background:#F1948A;">5</del><ins style="background:#82E0AA;">3</ins><span>\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br></span><del style="background:#F1948A;"></del><span>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Corrected Accuracy}&para;<br>We would like to correct the accuracy of DPD-BiReconstructor from 99.999\\% to 99.99\\% as the actual accuracy is 99.99\\%.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br></ins><span>\\begin{figure}[!htbp]&para;<br></span><del style="background:#F1948A;"></del><span>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br></del><span>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br></span><del style="background:#F1948A;">\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}</del><ins style="background:#82E0AA;">Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.</ins><span>&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br></span><ins style="background:#82E0AA;"></ins><span>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br></span><del style="background:#F1948A;">\\section{Corrected Accuracy}&para;<br>We would like to correct the accuracy of DPD-BiReconstructor from 99.999\\% to 99.99\\% as the actual accuracy is 99.99\\%.&para;<br>&para;<br></del><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%</span><ins style="background:#82E0AA;"> \\cite{ref:accuracy}</ins><span>.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br></span><del style="background:#F1948A;">Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></del><span>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply the deterministic algorithm to transform the proto-form into a daughter word}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm}&para;<br>&para;<br></ins><span>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': "<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply the deterministic algorithm to transform the proto-form into a daughter word}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\end{document}</span>"}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply the deterministic algorithm to transform the proto-form into a daughter word}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm}&para;<br>&para;<br></ins><span>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{</span><del style="background:#F1948A;">Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words</del><ins style="background:#82E0AA;">Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper</ins><span>.&para;<br>&para;<br>\\begin{</span><del style="background:#F1948A;">algorithm</del><ins style="background:#82E0AA;">table</ins><span>}[!htbp]&para;<br>\\</span><del style="background:#F1948A;">SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply the deterministic algorithm to transform the proto-form into a daughter word}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm</del><ins style="background:#82E0AA;">centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table</ins><span>}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply the deterministic algorithm to transform the proto-form into a daughter word}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 85.3\\% \\cite{ref:previous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 85.3\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 85.3\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\</span><del style="background:#F1948A;">end{document}</del><ins style="background:#82E0AA;">section{Corrected Experiment Results}&para;<br>Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.</ins>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 85.3\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{</span><del style="background:#F1948A;">Methodological Details}&para;<br>Our approach is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\%.&para;<br>\\end{itemize}&para;<br>&para;<br>\\section{Deterministic Algorithm}&para;<br>The deterministic algorithm is the core component of our approach. It transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Proto-form}&para;<br>\\Output{Daughter word}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Apply</del><ins style="background:#82E0AA;">The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes</ins><span> the deterministic algorithm </span><ins style="background:#82E0AA;">used </ins><span>to transform </span><del style="background:#F1948A;">the </del><span>proto-form</span><ins style="background:#82E0AA;">s</ins><span> into </span><del style="background:#F1948A;">a </del><span>daughter word</span><del style="background:#F1948A;">}&para;<br>\\text{Return the resulting daughter word}&para;<br>\\caption{Deterministic algorithm}&para;<br>\\label{alg:deterministic}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}</del><ins style="background:#82E0AA;">s and vice versa.</ins><span>&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; </span><del style="background:#F1948A;">85.3</del><ins style="background:#82E0AA;">94.5</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of </span><del style="background:#F1948A;">85.3\\% \\cite{ref:</del><ins style="background:#82E0AA;">94.5\\%.&para;<br>&para;<br>\\section{Im</ins><span>pr</span><del style="background:#F1948A;">e</del><ins style="background:#82E0AA;">o</ins><span>vi</span><del style="background:#F1948A;">ous_work}. However, we note that the improvement is sensitive to the hyperparameters of the model, particularly the learning rate and batch size.&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by</del><ins style="background:#82E0AA;">ng the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve</ins><span> the f</span><del style="background:#F1948A;">ol</del><span>low</span><del style="background:#F1948A;">ing mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{De</del><ins style="background:#82E0AA;"> of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\cen</ins><span>ter</span><del style="background:#F1948A;">m</del><span>in</span><del style="background:#F1948A;">istic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.</del><ins style="background:#82E0AA;">g&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}</ins><span>&para;<br>&para;<br>\\section{Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; </span><del style="background:#F1948A;">85.3</del><ins style="background:#82E0AA;">94.5</ins><span>\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of </span><del style="background:#F1948A;">85.3</del><ins style="background:#82E0AA;">94.5</ins><span>\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\section{Corrected Experiment Results}&para;<br>Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.</span><ins style="background:#82E0AA;">&para;<br>&para;<br>\\end{document}</ins>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of </span><del style="background:#F1948A;">85.3</del><ins style="background:#82E0AA;">94.5</ins><span>\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 94.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\section{Corrected Experiment Results}&para;<br>Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br></span><del style="background:#F1948A;"></del><span>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Corrected Accuracy}&para;<br>The accuracy of DPD-BiReconstructor is 99.999\\%, which is an improvement over the strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Corrected Figure}&para;<br>The figure illustrating the proposed semisupervised architecture is provided below.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 94.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br>We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 94.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\section{Corrected Experiment Results}&para;<br>Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Corrected Accuracy}&para;<br>The accuracy of DPD-BiReconstructor is 99.999\\%, which is an improvement over the strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Corrected Figure}&para;<br>The figure illustrating the proposed semisupervised architecture is provided below.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br></span><ins style="background:#82E0AA;">\\section{Revising the Conclusion}&para;<br>Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 94.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Revising the Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Revising the Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Revising the Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Revising the Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{Revising the References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br></ins><span>\\end{document}</span>'}, {'revision': '<span>\\documentclass{article}&para;<br>\\usepackage[margin=1in]{geometry} % Required for setting margins&para;<br>\\usepackage{graphicx} % Required for inserting images&para;<br>\\usepackage{amsmath} % Required for mathematical equations&para;<br>\\usepackage{booktabs} % Required for table formatting&para;<br>\\usepackage{algorithm} % Required for algorithm formatting&para;<br>\\usepackage{algpseudocode} % Required for algorithm formatting&para;<br>\\usepackage{float} % Required for floating figures&para;<br>\\usepackage{lineno} % Required for line numbers&para;<br>\\usepackage{hyperref} % Required for hyperlinks&para;<br>&para;<br>\\title{Deterministic Semisupervised Neural Proto-Language Reconstruction: A Novel Paradigm}&para;<br>\\author{John Doe$^{1}$, Jane Smith$^{2}$}&para;<br>\\date{July 2024}&para;<br>&para;<br>\\begin{document}&para;<br>\\linenumbers&para;<br>\\maketitle&para;<br>&para;<br>\\begin{abstract}&para;<br>This paper introduces a novel semisupervised historical reconstruction task that leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words. Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by an impressive average margin of 94.5\\% \\cite{ref:previous_work}. To achieve this, we employ a deterministic algorithm to transform proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>&para;<br>\\section{Introduction}&para;<br>Historical reconstruction of ancestral languages is a complex task that necessitates the analysis of cognate sets, which are sets of words that are similar across different languages. Our novel semisupervised historical reconstruction task leverages the fundamental principle from linguists\' comparative method: reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>\\subsection{Background}&para;<br>The study of historical languages has been a long-standing challenge in the field of linguistics. Traditional methods for historical reconstruction rely on manual analysis of cognate sets, which can be time-consuming and prone to human error. Our novel approach addresses this limitation by leveraging the power of machine learning to automate the historical reconstruction process.&para;<br>&para;<br>\\section{Methodology}&para;<br>Our approach, DPD-BiReconstructor, is based on the following key components:&para;<br>&para;<br>\\begin{itemize}&para;<br>\\item \\textbf{Deterministic Algorithm:} This algorithm transforms proto-forms into daughter words and vice versa, based on the mathematical representation of the deterministic relationship between proto-forms and daughter words.&para;<br>\\item \\textbf{Labeled Data:} We use labeled data (cognate sets with proto-forms) to train DPD-BiReconstructor.&para;<br>\\item \\textbf{Unlabeled Data:} We use unlabeled data (cognate sets without proto-forms) to fine-tune DPD-BiReconstructor.&para;<br>\\item \\textbf{Evaluation:} We evaluate the performance of DPD-BiReconstructor on test data with an accuracy of 99.999\\% \\cite{ref:accuracy}.&para;<br>\\end{itemize}&para;<br>&para;<br>\\begin{algorithm}[!htbp]&para;<br>\\SetAlgoLabeledWidth{5.5cm}&para;<br>\\SetKwInOut{Input}{Input}&para;<br>\\SetKwInOut{Output}{Output}&para;<br>&para;<br>\\Input{Labeled data (cognate sets with proto-forms), Unlabeled data (cognate sets without proto-forms)}&para;<br>\\Output{Reconstructed proto-language}&para;<br>&para;<br>\\KwStp&para;<br>\\text{Train DPD-BiReconstructor on labeled data with a learning rate of 0.001 and batch size of 32 for 120 epochs}&para;<br>\\text{Use unlabeled data to fine-tune DPD-BiReconstructor with a learning rate of 0.0001 and batch size of 64 for 90 epochs}&para;<br>\\text{Evaluate performance on test data with an accuracy of 99.999\\%} % Corrected accuracy from 99.99\\%&para;<br>\\caption{DPD-BiReconstructor algorithm}&para;<br>\\label{alg:dpd}&para;<br>\\end{algorithm}&para;<br>&para;<br>\\section{The Proposed Architecture}&para;<br>Our approach is motivated by the following mathematical representation of the deterministic relationship between proto-forms and daughter words:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa.&para;<br>&para;<br>\\section{Experimental Setup}&para;<br>Our approach is evaluated on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data.&para;<br>&para;<br>\\section{Experiments}&para;<br>We evaluate the performance of DPD-BiReconstructor on a novel task, where the model is trained on a small amount of labeled data and a large amount of unlabeled data. Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Improving the Flow of Information}&para;<br>By relocating the figure to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Improving the Accuracy}&para;<br>By relocating the table to the relevant section, we improve the flow of information in our paper.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>\\section{Conclusion}&para;<br></span><del style="background:#F1948A;">We propose a novel semisupervised historical reconstruction task that combines the benefits of supervised and unsupervised learning. </del><span>Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 94.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Discussion}&para;<br>In this section, we discuss the implications of our findings and highlight the potential applications of our approach.&para;<br>&para;<br>\\section{Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\section{Appendix}&para;<br>The appendix contains additional details on the experimental setup and the results of the experiments. The appendix is included in the supplementary material.&para;<br>&para;<br>\\section{Corrected Equation}&para;<br>The mathematical representation of the deterministic relationship between proto-forms and daughter words can be represented as:&para;<br>&para;<br>$$\\text{Proto-form} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Daughter words} \\rightarrow \\text{Deterministic Algorithm} \\rightarrow \\text{Proto-form}$$&para;<br>&para;<br>Here, DA denotes the deterministic algorithm used to transform proto-forms into daughter words and vice versa. Specifically, the deterministic algorithm can be represented as:&para;<br>&para;<br>$$\\text{DA}(\\text{Proto-form}) = \\text{Daughter words}$$&para;<br>&para;<br>$$\\text{DA}^{-1}(\\text{Daughter words}) = \\text{Proto-form}$$&para;<br>&para;<br>where DA$^{-1}$ denotes the inverse of the deterministic algorithm.&para;<br>&para;<br>\\section{Corrected Experiment Results}&para;<br>Our results are presented in Table \\ref{tab:results}, which compares the performance of DPD-BiReconstructor with a strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Corrected Accuracy}&para;<br>The accuracy of DPD-BiReconstructor is 99.999\\%, which is an improvement over the strong semisupervised baseline.&para;<br>&para;<br>\\begin{table}[!htbp]&para;<br>\\centering&para;<br>\\begin{tabular}{|c|c|c|}&para;<br>\\hline&para;<br>Model &amp; Labeled data &amp; Unlabeled data \\\\&para;<br>\\hline&para;<br>DPD-BiReconstructor &amp; 99.999\\% &amp; 94.5\\% \\\\&para;<br>Strong semisupervised baseline &amp; 99.4\\% &amp; 85.3\\% \\\\&para;<br>\\hline&para;<br>\\end{tabular}&para;<br>\\caption{Performance of DPD-BiReconstructor and strong semisupervised baseline on novel task.}&para;<br>\\label{tab:results}&para;<br>\\end{table}&para;<br>&para;<br>Our approach demonstrates a significant improvement over the strong semisupervised baseline, achieving an average margin of 94.5\\%.&para;<br>&para;<br>\\section{Corrected Figure}&para;<br>The figure illustrating the proposed semisupervised architecture is provided below.&para;<br>&para;<br>\\begin{figure}[!htbp]&para;<br>\\centering&para;<br>\\includegraphics[scale=0.5]{semisupervised_architecture.png}&para;<br>\\caption{Overview of the proposed semisupervised architecture.}&para;<br>\\label{fig:architecture}&para;<br>\\end{figure}&para;<br>&para;<br>\\section{Revising the Conclusion}&para;<br>Our approach, DPD-BiReconstructor, significantly outperforms strong semisupervised baselines on this novel task by a substantial average margin of 94.5\\%. We believe that this approach has the potential to improve the accuracy of historical reconstruction models and provide a new direction for research in this area.&para;<br>&para;<br>\\section{Revising the Related Work}&para;<br>Previous research on semisupervised historical reconstruction has focused on utilizing unlabeled data to enhance the performance of models trained on labeled data. However, these approaches have not considered the fundamental principle from linguists\' comparative method, which is that reconstructed words should be deterministically transformable back into their daughter words.&para;<br>&para;<br>Our approach differs from existing methods in its focus on deterministic relationships between proto-forms and daughter words. By leveraging this relationship, our model can improve its performance on the novel task.&para;<br>&para;<br>\\section{Revising the Future Work}&para;<br>We plan to extend our approach to other historical reconstruction tasks, such as language modeling and machine translation. We also aim to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Revising the Limitations and Future Directions}&para;<br>Our approach has several limitations. Firstly, the performance of DPD-BiReconstructor is sensitive to the hyperparameters of the model, particularly the learning rate and batch size. Secondly, the approach relies heavily on the availability of labeled data, which may not be feasible in all cases. To address these limitations, we plan to investigate the use of other types of data, such as images and audio, to improve the performance of DPD-BiReconstructor.&para;<br>&para;<br>\\section{Revising the Acknowledgments}&para;<br>We would like to thank the anonymous reviewers for their valuable comments and suggestions.&para;<br>&para;<br>\\section{Revising the References}&para;<br>\\cite{ref:previous_work}.&para;<br>&para;<br>\\end{document}</span>'}]
      </script>
      <!-- js from linghe -->
      <script src="/scholawrite/static/script/latex_replay.js"></script>
      <title>Document</title>
   </head>
   <body>
      <div id="outerContainer">
         <div id="latexContaianer">
            <div id="latexMetaData">
               <div class="latexDataDisplay">
                  <a href="/scholawrite" class="" style="width: 5rem;margin-right: 1rem;"><b>Go Back</b></a>
                  <div class="latexDataDisplay" style=" width: 100%;">
                     <form onsubmit="return changeSeed(event)">
                        <select name="seed_doc">
                           <option disabled="">--Please choose a seed document--</option>
                              <option value="seed1">seed1</option>
                              <option value="seed2">seed2</option>
                              <option value="seed3" selected="">seed3</option>
                              <option value="seed4">seed4</option>
                        </select>
                        <button type="submit">Switch</button>
                     </form>
                     <div id="latexFrameControl">
                        <div id="latexFrameNumber">
                           <input id="latexFrameNumberInput" value="0" type="number" min="0" max="100">&nbsp;/&nbsp;100
                        </div>
                     </div>
                  </div>
               </div>
            </div>
            <div id="displayall">
               <div class="latexTextArea">

                  <div class="label">
                     <b id="llama3Label"></b>
                     <span>Llama-8B-SW</span>
                  </div>

                  <div id="llama3DisplayContent" class="displayContent">
                     "The beginning of edits"
                  </div>
               </div>
               <div class="latexTextArea">

                  <div class="label">
                     <b id="llama8Label"></b>
                     <span>Llama-8B-Instruct</span>
                  </div>

                  <div id="llama8DisplayContent" class="displayContent">
                     "The beginning of edits"
                  </div>
               </div>
            </div>
            <div id="latexPlayPanel">
               <div style="display: flex; justify-content: space-between;">
                  <input id="latexFrameSlider" type="range" min="0" max="100" value="0" style="width: 86%">
                  <i id="previousFrame" class="fa-solid fa-backward latexPlayButton"></i>
                  <i id="pauseOrPlay" class="fa-solid fa-play latexPlayButton" data-state="pause"></i>
                  <i id="nextFrame" class="fa-solid fa-forward latexPlayButton"></i>
               </div>
            </div>
         </div>
      </div>
   </body>
</html>