\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{footnote}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{multirow}

\title{Semisupervised Neural Proto-Language Reconstruction}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Existing research in comparative reconstruction of ancestral languages, or proto-languages, has traditionally depended on full supervision \citep{bouchard2013automated}. However, practicality demands the development of models that require minimal labeled data for training. We introduce a semisupervised historical reconstruction framework in which the model is trained on a small, labeled dataset—cognate sets with proto-forms—and a large corpus of unlabeled data comprising cognate sets without proto-forms. Our approach employs a neural architecture for comparative reconstruction known as the DPD-BiReconstructor \citep{list2019beyond}. This technique is rooted in a crucial understanding from the linguists' comparative method: reconstructed words should not only originate from, but also be transformable into their descendant forms deterministically \citep{campbell2013historical}. By leveraging this principle, we show that our architecture can utilize unlabeled cognate sets effectively, surpassing strong semisupervised baselines \citep{vincent2008extracting} and opening new avenues for proto-language reconstruction.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
The quest for reconstructing proto-languages has been a core objective in historical linguistics, significantly advanced through computational techniques \citep{bakker2006introduction}. Traditional methods required labor-intensive manual efforts, relying on fully supervised data, often unavailable for many language families \citep{campbell2013historical}. Our work introduces a semisupervised model leveraging both labeled and unlabeled data, addressing the data scarcity challenge, as illustrated in Figure \ref{fig:data_challenge}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{data_challenge.png}
    \caption{The challenge of data scarcity in proto-language reconstruction and the proposed solution. The diagram depicts the disparity between available labeled and unlabeled data in typical linguistic datasets.}
    \label{fig:data_challenge}
\end{figure}

\section{Related Work}
\label{sec:related_work}
Proto-language reconstruction tasks have utilized a variety of methodologies, from rule-based systems to modern neural network architectures \citep{bouchardcote2013automated}. Table \ref{tab:related_methods} compares these methodologies. Historically, these approaches have focused on fully supervised learning \citep{list2020automatic}. Our research extends these efforts by incorporating semisupervised learning, an area less explored in this field \citep{zhu2005semi}.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
        \toprule
        Method & Supervision Requirement & Key Advantage \\
        \midrule
        Rule-based & High & Interpretability \\
        Neural Networks & High & Scalability \\
        \textbf{Semisupervised (ours)} & \textbf{Low} & \textbf{Data Efficiency} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of methodologies for proto-language reconstruction, highlighting the distinctive advantages and resource requirements of each approach.}
    \label{tab:related_methods}
\end{table}

\section{Methodology}
\label{sec:methodology}

\subsection{Task Definition}
\label{subsec:task_definition}
Our semisupervised reconstruction task is structured to predict proto-forms from a partially labeled dataset of cognate pairs and a more extensive unlabeled dataset. The labeled data consists of cognate pairings with established proto-language equivalents from linguistic databases. The mathematical objective is expressed in Equation \ref{eq:objective}.

\begin{equation}
    P(\text{Proto-Form} \mid \text{Labeled Cognates}) \times P(\text{Proto-Form} \mid \text{Unlabeled Cognates})
    \label{eq:objective}
\end{equation}

This task utilizes limited labeling to estimate proto-forms for unlabeled cognates, detailed in the Model Architecture in Section \ref{subsec:model_architecture}.

\subsection{Model Architecture}
\label{subsec:model_architecture}
The DPD-BiReconstructor model applies a bi-directional recurrent architecture, taking inspiration from sequence-to-sequence models in neural machine translation \citep{sutskever2014sequence}. Figure \ref{fig:model_architecture} demonstrates how this architecture is tailored for historical linguistic tasks, naturally aligning with transformation constraints inherent to the comparative linguistic method \citep{mccarthy2016improving}. Section \ref{sec:experiments} will further explore the performance and applicability of this model.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{model_architecture.png}
    \caption{Architecture of the DPD-BiReconstructor model. The model capitalizes on the bidirectional flow of information, crucial for encoding linguistic transformation paths.}
    \label{fig:model_architecture}
\end{figure}

\section{Experiments}
\label{sec:experiments}

\subsection{Data}
\label{subsec:data}
Our experiments draw upon datasets sourced from publicly available linguistic resources, representing diverse language families. Table \ref{tab:dataset_split} illustrates the dataset partitioning into training and testing sets, where labeled data forms a minor segment, reflecting real-world data availability conditions \citep{mayer2014ling}. The impact of this data distribution is examined in Section \ref{subsec:results}.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Dataset Split & Labeled & Unlabeled \\
        \midrule
        Training & 10\% & 90\% \\
        Testing & 30\% & 70\% \\
        \bottomrule
    \end{tabular}
    \caption{Dataset composition used in experiments, demonstrating the proportionate distribution between labeled and unlabeled datasets.}
    \label{tab:dataset_split}
\end{table}

\subsection{Training Setup}
\label{subsec:training_setup}
The training of the DPD-BiReconstructor employs cross-entropy loss for supervised data along with consistency regularization for unsupervised data. The integrated loss function is outlined in Equation \ref{eq:loss_function}. Optimization is conducted using the Adam optimizer, complemented by batch normalization to ensure learning stability across diverse data distributions \citep{ioffe2015batch}, as further depicted in Section \ref{sec:results}.

\begin{equation}
    \text{Loss} = \text{Cross-Entropy} + \lambda \times \text{Consistency Regularization}
    \label{eq:loss_function}
\end{equation}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Learning rate & 0.001 \\
        Batch size & 64 \\
        $\lambda$ (Regularization) & 0.5 \\
        \bottomrule
    \end{tabular}
    \caption{Hyperparameters used during training the DPD-BiReconstructor model.}
    \label{tab:training_parameters}
\end{table}

\subsection{Results}
\label{subsec:results}
As depicted in Figure \ref{fig:accuracy_comparison}, our DPD-BiReconstructor demonstrates superior predictive performance compared to traditional fully supervised models in a semisupervised context, achieving significant accuracy gains. These findings highlight the promise of semisupervised strategies for advancing proto-language reconstruction \citep{kingma2014semi}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{accuracy_comparison.png}
    \caption{Predictive accuracy between traditional and semisupervised approaches. The DPD-BiReconstructor shows improved results due to effective utilization of unlabeled data.}
    \label{fig:accuracy_comparison}
\end{figure}

\section{Discussion}
\label{sec:discussion}
Our model underscores the harmony between neural architectures and linguistic theory, advocating for hybrid approaches that utilize empirical data and theoretical insights. Figure \ref{fig:data_theory_synergy} visualizes this synergy, demonstrating how our model benefits from both components to enhance reconstruction outputs \citep{mariani2015impact}. This complements the findings presented in Section \ref{subsec:results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{data_theory_synergy.png}
    \caption{Data-driven approaches and linguistic theory synergy in our model. This visualization captures the interdependent relationship between data inputs and theoretical foundations, essential for improved reconstruction quality.}
    \label{fig:data_theory_synergy}
\end{figure}

\section{Conclusion}
\label{sec:conclusion}
Our research presents a groundbreaking semisupervised approach to proto-language reconstruction, leveraging a neural architecture that aligns with linguistic principles to utilize both labeled and unlabeled data effectively. This framework establishes a foundation for scalable and efficient historical linguistic analysis, as discussed in Section \ref{sec:discussion}.

\begin{itemize}
    \item Develop better integration of linguistic theory in machine learning models \citep{smolensky1996harmonic}.
    \item Extend the dataset to cover more language families for comprehensive testing \citep{attanasi2020language}.
    \item Explore the impact of varying the proportion of labeled to unlabeled data \citep{chapelle2006semi}.
\end{itemize}

\section*{Acknowledgments}
Acknowledging support from linguistic data sources and computational resources facilitating this research.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document} 