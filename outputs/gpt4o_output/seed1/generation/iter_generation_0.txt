\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{How Johnny Can Persuade LLMs to Jailbreak Them: 
\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused 
attacks developed by security experts. As \textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  
explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. 
First, we propose a persuasion taxonomy derived from decades of social science research.
Then we apply the taxonomy to automatically generate 
interpretable \textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. 
Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent 
algorithm-focused attacks. 
On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for 
more fundamental mitigation for highly interactive LLMs.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Artificial Intelligence (AI) safety research traditionally has concentrated on algorithm-focused attacks. However, with the widespread usage of Large Language Models (LLMs), non-expert user interactions pose new risks. This paper rethinks AI safety by humanizing LLMs and examines how effective persuasion can lead them to jailbreak. We explore the intersection of natural language interactions and AI safety concerns. 

\section{Background and Related Work}
\label{sec:background}
This section situates our work within the existing research landscape, providing an overview of AI safety with an emphasis on LLM vulnerabilities. We review significant advancements in jailbreak attempts and examine existing defense mechanisms.

\subsection{AI Safety and LLMs}
\label{subsec:ai_safety}
AI safety attempts to mitigate the risks posed by advanced AI systems. Recent work focuses predominantly on algorithmic defenses while largely overlooking the linguistic challenges presented by interactive systems such as LLMs.

\subsection{Persuasion in Human-Computer Interaction}
\label{subsec:persuasion}
Drawing on social science principles, we present the role of persuasion in influencing both human judgments and computational decisions, emphasizing its potential impact on LLMs.

\section{Persuasion Taxonomy}
\label{sec:taxonomy}
We develop a comprehensive persuasion taxonomy rooted in social science research, understanding argument strategies and their application in convincing LLMs. 

\section{Methodology}
\label{sec:methodology}
This section elaborates on our approach to crafting \textit{Persuasive Adversarial Prompts} (PAP) based on the derived persuasion taxonomy.

\section{Experimental Setup}
\label{sec:experimental_setup}
Here, we discuss the parameters and configurations used in our assessments, including models, datasets, and evaluation metrics.

\section{Results and Analysis}
\label{sec:results}
Our experiments demonstrate that PAP leads to significantly higher success rates in jailbreaking LLMs compared to traditional attacks. We analyze the results across various models and scenarios.

\section{Defense Mechanisms}
\label{sec:defense}
We explore several defense strategies to counteract PAP and identify shortcomings in current systems. Recommendations for future protection frameworks are discussed.

\section{Conclusion}
\label{sec:conclusion}
In conclusion, our research illustrates the potential risk of persuasion-based attacks on LLMs and emphasizes the need for robust, human-aware defensive mechanisms in AI models.

\bibliographystyle{plain}
\bibliography{references}

\end{document}