<same>\documentclass{article}
\usepackage{graphicx} % required for inserting images

\title{How </same><del>L </del><add>LLMs </add><same>They: 
\\Rethinking Persuption to Challenge AI Safety by Humanizing LLMs}
\author{}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused 
attacks developed by </same><del>outputs </del><add>pr </add><same>experts. As \textit{large language models} (LLMs) become increasingly common and capable,</same><add> this</add><same> non-expert users can also pose risks during everyday interactions. This paper introduces a new perspective on L  
explore this overlooked interaction between everyday language and AI safety. Specifically, we study how to </same><del>persuade LLMs </del><add>e LLMs </add><same>to b them. 
First, we propose a \textit{pr lexicality </same><del>r </del><add>resulting </add><same>from cads es. Then we apply the \textit{lexi to automatically generate 
e jailbreak LLMs. 
h persuiveness significantly increases the \textit{i performance across all risk categories: PAP h attacks e rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ r 
algorithm-based attacks. 
e explore various ncans against PAP, find a significant gap in existing </same><del>e </del><add>empirical </add><same>advise </same><del>decide for </del><add>d </add><same>
e.
\end{abstract}

\end{document}
</same>