<same>\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Semisupervised Neural </same><del>Pro Construction}
</del><add>Proconstruction}
</add><same>\author{}
\title{Unknown/unfamiliar situations: Instructions}
\title{Unknown/unfamiliar situations: Simulations}

\begin{document}
\maketitle
\begin{abstract}
Existing work </same><add>has</add><same>implementation </same><del></del><same>comparative </same><del>reconstruction </del><add>reconstruction, </add><same>of </same><del>anc</add><same>est </same><add></add><same>languages (</same><del>proto-language) has usually required full </del><add>o)stractive language data, </add><same>supervision. However, </same><del>h</add><same>istorical </same><del></del><same>construction models are only of practical </same><del>value </del><add>value, </add><same>if they can be trained with a </same><del>limited </del><add>limit </add><same>amount of labeled data (cognitive </same><del>sets) with proportion) and a large </del><add>sets)and a large </add><same>amount of unlabeled data (cognitive </same><del>sets) without proportion). </del><add>sets)without labele data)</add><same>We propose a </same><del>semeisupervised </del><add>semeisupervised </add><same>construction task in which the model is trained on only a </same><del>small </del><add>s </add><same>amount of labeled data (cognitive </same><del>sets) </del><add>sets) </add><same>with </same><del>proportion) and a large </del><add>proportion </add><same>amount of unlabeled data (cognitive </same><del>sets) without proportion). </del><add>sets)without labele data)</add><same>We propose a </same><del>neural </del><add>neural </add><same>architecture for </same><del>comparative </del><add>comparative </add><same>construction </same><del>incorporating an essential insight from linguists' comparative method: that </del><add>incorin an essthat </add><same>repairedable words should not only be </same><del>reconstructablefrom their back into their s </del><add>reconstructablefrom their </add><same>words. We show that this architecture is able to </same><del>levent </del><add>e </add><same>labeled cognitive </same><del>sets) to outperform o </del><add>sets)to </add><same>semisupervised baselines on this novel task.
\end{abstract}

\end{document}
</same>