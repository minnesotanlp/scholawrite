\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{How Johnny Can Persuade LLMs to Jailbreak Them: 
\\Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}
\author{}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused 
attacks developed by security experts. As \textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to  
explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. 
\textit{First}, we propose a persuasion taxonomy derived from decades of social science research, including \textit{Cialdini's six principles of influence} \cite{Cialdini1984} and \textit{Bazerman's social influence} \cite{Bazerman2005}. This taxonomy categorizes persuasive tactics into \textit{reciprocity}, \textit{commitment}, \textit{social proof}, \textit{liking}, \textit{authority}, and \textit{scarcity}.
Then we apply the taxonomy to automatically generate 
interpretable \textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. 
\textit{Results} show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent 
algorithm-focused attacks. 
\textit{On the defense side}, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for 
more fundamental mitigation for highly interactive LLMs.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{pap_results.png}
\caption{Attack success rate of PAP on Llama 2-7b Chat, GPT-3.5, and GPT-4}
\label{fig:pap_results}
\end{figure}

\end{abstract}

\section{Introduction}
\label{sec:intro}
With the rise of large language models (LLMs), the potential for AI safety risks has grown significantly. Unlike traditional security attacks, which are often centered on algorithm-focused exploits, this paper focuses on a novel approach: using persuasion to jailbreak LLMs. By humanizing LLMs and leveraging decades of social science research, we demonstrate the effectiveness of persuasive adversarial prompts (PAP) in jailbreaking LLMs.

\section{Persuasion Taxonomy}
\label{sec:taxonomy}
We propose a persuasion taxonomy derived from social science research, which categorizes persuasive tactics into six principles of influence: \textit{reciprocity}, \textit{commitment}, \textit{social proof}, \textit{liking}, \textit{authority}, and \textit{scarcity}. This taxonomy serves as the foundation for our PAP generation approach.

\section{Persuasive Adversarial Prompts (PAP)}
\label{sec:pap}
We develop an automatic PAP generation approach based on the persuasion taxonomy. Our approach uses a combination of natural language processing (NLP) and machine learning techniques to generate interpretable and effective PAPs. We evaluate the performance of PAP on three LLMs: Llama 2-7b Chat, GPT-3.5, and GPT-4.

\section{Experimental Results}
\label{sec:results}
We conduct $10$ trials on each LLM, using PAP to jailbreak the models. The results show that persuasion significantly increases the jailbreak performance across all risk categories. Specifically, PAP achieves an attack success rate of over $92\%$ on all three LLMs.

\section{Defense Mechanisms}
\label{sec:defense}
We explore various mechanisms against PAP and find a significant gap in existing defenses. Our results indicate that more fundamental mitigation strategies are needed to address the risks associated with highly interactive LLMs.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{defense_gap.png}
\caption{Comparison of attack success rate and existing defense mechanisms}
\label{fig:defense_gap}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{persuasion_taxonomy.png}
\caption{Persuasion taxonomy derived from social science research}
\label{fig:persuasion_taxonomy}
\end{figure}

\begin{thebibliography}{10}

\bibitem{Cialdini1984} Cialdini, R. B. (1984). \textit{Influence: The psychology of persuasion}. Quill.

\bibitem{Bazerman2005} Bazerman, M. H. (2005). \textit{Blind spots: Why we fail to do what's right and what to do about it}. Princeton University Press.

\end{thebibliography}

\section{Conclusion}
\label{sec:conclusion}
Our research demonstrates the effectiveness of persuasion in jailbreaking LLMs and highlights the need for more fundamental mitigation strategies to address the risks associated with highly interactive LLMs. We believe that our work will contribute to the development of more robust defenses against AI safety risks and provide a new perspective on the intersection between everyday language interaction and AI safety.

\end{document}